import os
import sys
import tempfile
import json
import time
import shutil
import traceback
import html2text
import argparse
from dotenv import load_dotenv
from llama_index.core import SimpleDirectoryReader, Document
from llama_index.core import Settings
from llama_index.core.readers.base import BaseReader
from llama_index.core.node_parser import HTMLNodeParser, SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.extractors import TitleExtractor, KeywordExtractor
from llama_index.core.schema import BaseNode, TextNode
from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader, 
    StorageContext,
    Document
)
from llama_index.core.storage.docstore.simple_docstore import SimpleDocumentStore
from llama_index.core.storage.index_store.simple_index_store import SimpleIndexStore
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.vector_stores.milvus import MilvusVectorStore
# from sentence_transformers import SentenceTransformer
from bs4 import BeautifulSoup
from pymilvus import MilvusClient
from huggingface_hub import snapshot_download
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from milvus_lite.server import Server
import data_cleaner


if os.path.exists('../../.env'):
    load_dotenv('../../.env')

# Default values
DEFAULT_DB_URI = 'http://localhost:19530'
DEFAULT_DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'data', 'zh-cn')
DEFAULT_MD_CACHE = "./md_cache"
DEFAULT_MILVUS_DATA = "milvus_data"
DEFAULT_START_MILVUS = False

milvus_server = None


def init_embed_model():
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–è®¾ç½®
    hf_endpoint = os.getenv("HF_ENDPOINT", "https://hf-mirror.com")
    hf_base_url = os.getenv("HUGGINGFACE_HUB_BASE_URL", "https://hf-mirror.com")
    model_name = os.getenv("EMBEDDING_MODEL", "BAAI/bge-small-en-v1.5")
    device = os.getenv("EMBEDDING_DEVICE", "cpu")
    backend = os.getenv("EMBEDDING_BACKEND", "torch")  # torch, onnx, openvino
    cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'hf_cache')
    cache_dir = os.getenv("EMBEDDING_CACHE_DIR", cache_dir)
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    model_path = model_name
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æ¨¡å‹ç›®å½•
    local_dir = os.getenv("EMBEDDING_LOCAL_DIR", "").strip()
    if local_dir and os.path.isdir(local_dir):
        model_path = local_dir
        print(f"Using local model: {model_path}")
    else:
        # å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œå°è¯•HuggingFaceçš„ç¼“å­˜æˆ–ä¸‹è½½
        if not os.path.exists(cache_dir):
            os.mkdir(cache_dir)
        print(f"Trying to download {model_name} to {cache_dir} from {hf_endpoint}")
        model_path = snapshot_download(
            model_name,
            endpoint=hf_endpoint,
            cache_dir=cache_dir
        )
        print(f"model downloaded to {model_path}")
    
    print(f"Load embedding model")
    print(f"model_path: {model_path}")
    print(f"device: {device}")
    print(f"backend: {backend}")
    
    # æ ¹æ®åç«¯ç±»å‹åˆå§‹åŒ–æ¨¡å‹
    if backend == "onnx":
        # ONNXåç«¯é…ç½®
        model_kwargs = {}
        
        # æ ¹æ®è®¾å¤‡é€‰æ‹©æ‰§è¡Œæä¾›è€…
        if device == "cuda":
            try:
                import onnxruntime as ort
                providers = ort.get_available_providers()
                if 'CUDAExecutionProvider' in providers:
                    model_kwargs["provider"] = "CUDAExecutionProvider"
                    print("Using CUDA ONNX execution provider")
                else:
                    print("CUDA execution provider not available, falling back to CPU")
                    model_kwargs["provider"] = "CPUExecutionProvider"
            except Exception as e:
                print(f"Error checking CUDA providers: {e}, falling back to CPU")
                model_kwargs["provider"] = "CPUExecutionProvider"
        else:
            # CPUæ¨¡å¼ï¼Œå¼ºåˆ¶ä½¿ç”¨CPUæ‰§è¡Œæä¾›è€…
            model_kwargs["provider"] = "CPUExecutionProvider"
            print("Using CPU ONNX execution provider")
        
        # æ£€æŸ¥ONNXæ¨¡å‹è·¯å¾„
        onnx_path = os.path.join(model_path, "onnx")
        if not os.path.exists(onnx_path):
            print(f"ONNX model path not found: {onnx_path}")
            print("Falling back to PyTorch backend")
            # å›é€€åˆ°PyTorchåç«¯
            Settings.embed_model = HuggingFaceEmbedding(
                model_name=model_path,
                device=device,
                backend="torch"
            )
        else:
            Settings.embed_model = HuggingFaceEmbedding(
                model_name=onnx_path,
                device=device,
                backend="onnx",
                model_kwargs=model_kwargs
            )
    else:
        # PyTorchåç«¯ï¼ˆé»˜è®¤ï¼‰
        Settings.embed_model = HuggingFaceEmbedding(
            model_name=model_path,
            device=device,
            backend="torch"
        )


def init_llm():
    # ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®è‡ªå®šä¹‰ç½‘å…³
    base_url = os.getenv("LLM_BASE_URL")
    api_key = os.getenv("LLM_API_KEY")
    model_name = os.getenv("LLM_MODEL", "qwen-plus")
    Settings.llm = OpenAI(model=model_name, base_url=base_url, api_key=api_key)


def start_milvus(milvus_data_path=DEFAULT_MILVUS_DATA, port=19530):
    global milvus_server
    
    try:
        print("ğŸš€ Starting Milvus server...")
        print(f"ğŸ“ Milvus data path: {milvus_data_path}")
        milvus_server = Server(db_file=milvus_data_path, address=f'localhost:{port}')
        milvus_server.start()
        print("âœ… Milvus server started successfully")
        
        # ç­‰å¾…æœåŠ¡å™¨å®Œå…¨å¯åŠ¨
        import time
        print("â³ Waiting for server to fully start...")
        time.sleep(3)
        
        # æµ‹è¯•è¿æ¥
        try:
            from pymilvus import connections
            connections.connect("default", host="localhost", port=port)
            print("âœ… Milvus connection test successful")
            connections.disconnect("default")
        except Exception as e:
            print(f"âš ï¸ Milvus connection test failed: {e}")
            
    except Exception as e:
        print(f"ğŸ”´ Milvus server startup failed: {e}")
        milvus_server = None
        raise


def cache_md(original_filename, md_content, data_dir, md_cache):
    rel_path = os.path.relpath(original_filename, data_dir)
    cache_path = os.path.join(md_cache, rel_path)
    base, ext = os.path.splitext(cache_path)
    cache_path = base + ".md"
    if not os.path.exists(os.path.dirname(cache_path)):
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
    with open(cache_path, 'w') as file:
        file.write(md_content)

def init_vector_store(db_uri):
    # åˆ›å»º MilvusVectorStore å®ä¾‹
    # åŠ¨æ€æ¨æ–­å‘é‡ç»´åº¦ä»¥åŒ¹é…å½“å‰åµŒå…¥æ¨¡å‹
    try:
        inferred_dim = len(Settings.embed_model.get_text_embedding("__dim_probe__"))
    except Exception:
        inferred_dim = 512  # BGE-small-zh-v1.5 512ç»´ç‰ˆæœ¬

    vector_store = MilvusVectorStore(
        uri=db_uri,                        # è¿æ¥åœ°å€
        dim=inferred_dim,                  # å‘é‡ç»´åº¦
        collection_name=os.environ['COLLECTION_NAME'],   # é›†åˆåç§°
        # enable_sparse=True,  # å¯ç”¨ç¨€ç–å‘é‡ï¼ˆBM25ï¼‰
        overwrite=True,                    # å¦‚æœé›†åˆå­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–
        consistency_level="Strong"         # ä¸€è‡´æ€§çº§åˆ«ï¼šStrong, Session, Bounded, Eventually
    )

    print("âœ… MilvusVectorStore initialized successfully!")
    print(f"ğŸ“Š Collection name: {vector_store.collection_name}")
    print(f"ğŸ”— Connection URI: {db_uri}")
    print(f"ğŸ“ Vector dimension: {inferred_dim}")
    return vector_store

# process single document with html2text
def process_doc_html2text(doc, data_dir, md_cache):
    h = html2text.HTML2Text()
    h.ignore_links = True
    soup = BeautifulSoup(doc.text, "html.parser")
    content = soup.find("div", class_="td-content")
    processed_content = h.handle(str(content))
    cache_md(doc.metadata['file_path'], processed_content, data_dir, md_cache)
    print(f"Processed file: {doc.metadata['file_path']}")
    # Document çš„ text å±æ€§æ˜¯åªè¯»çš„ï¼Œéœ€æ–°å»ºå¯¹è±¡å¹¶ä¿ç•™å…ƒæ•°æ®
    processed_doc = Document(text=processed_content, metadata=doc.metadata or {})
    return processed_doc

def process(data_dir, md_cache, db_uri=DEFAULT_DB_URI):
    vector_store = init_vector_store(db_uri)

    # ä½¿ç”¨æ›´çµæ´»çš„è¿‡æ»¤æ–¹å¼ï¼Œè¿‡æ»¤æ‰è·¯å¾„ä¸­åŒ…å« _print/index.html çš„æ–‡ä»¶
    exclude_files = []

    # è¿­ä»£å¼åŠ è½½å’Œå¤„ç†
    print(f"Going to load data from {data_dir}")
    reader_iter = SimpleDirectoryReader(
        input_dir=data_dir, 
        required_exts=[".html"],
        exclude=exclude_files,
        recursive=True
    )

    md_docs = []
    processed_files = 0

    print("Starting iterative loading:")
    for docs in reader_iter.iter_data():
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶çš„æ–‡æ¡£
        processed_docs = []
        for doc in docs:
            # è¿‡æ»¤æ‰è·¯å¾„ä¸­åŒ…å« _print/index.html çš„æ–‡ä»¶
            if doc.metadata and 'file_path' in doc.metadata:
                file_path = doc.metadata['file_path']
                if '_print/index.html' in file_path:
                    print(f"ğŸš« è·³è¿‡æ–‡ä»¶: {file_path} (åŒ…å« _print/index.html)")
                    continue
            
            # åœ¨è¿™é‡Œå¯ä»¥è¿›è¡Œå®æ—¶å¤„ç†ï¼Œæ¯”å¦‚æ•°æ®æ¸…æ´—ã€åˆ†æç­‰
            processed_doc = process_doc_html2text(doc, data_dir, md_cache)
            processed_docs.append(processed_doc)
            
        md_docs.extend(processed_docs)
        processed_files += 1
        print(f"Processed file {processed_files} documents")

    print(f"\nLoading completed, total {len(md_docs)} documents")

    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨æ˜¾å¼ç»„ä»¶é¿å…æ–‡ä»¶åŠ è½½é—®é¢˜
    # docstore = SimpleDocumentStore()
    # index_store = SimpleIndexStore()
    
    # storage_context = StorageContext.from_defaults(
    #     docstore=docstore,
    #     index_store=index_store,
    #     vector_store=vector_store
    # )
    
    # ä»æ–‡æ¡£åˆ›å»ºå‘é‡ç´¢å¼•
    print("\nğŸ“¥ Starting to build vector index...")
    print(f"ğŸ“ Vector store: {vector_store.collection_name}")
    
    index = VectorStoreIndex.from_documents(
        md_docs,
        storage_context=storage_context,
        show_progress=True
    )

    print("âœ… Vector index construction completed!")
    print(f"ğŸ“– Indexed {len(md_docs)} documents")
    return index


def main():
    """Main function to handle command line arguments and execute data processing"""
    parser = argparse.ArgumentParser(description='Process HTML documents and create vector index')
    parser.add_argument('--data-dir', type=str, default=DEFAULT_DATA_DIR,
                        help=f'Path to the data directory containing HTML files (default: {DEFAULT_DATA_DIR})')
    parser.add_argument('--md-cache', type=str, default=DEFAULT_MD_CACHE,
                        help=f'Path to store converted markdown files (default: {DEFAULT_MD_CACHE})')
    parser.add_argument('--start-milvus', action='store_true', default=DEFAULT_START_MILVUS,
                        help='Whether to start Milvus server (default: False)')
    parser.add_argument('--milvus-data', type=str, default=DEFAULT_MILVUS_DATA,
                        help=f'Path to Milvus database files (default: {DEFAULT_MILVUS_DATA})')
    parser.add_argument('--db-uri', type=str, default=DEFAULT_DB_URI,
                        help=f'Milvus database URI (default: {DEFAULT_DB_URI})')
    
    args = parser.parse_args()
    
    # Validate paths
    if not os.path.exists(args.data_dir):
        print(f"ğŸ”´ Error: Data directory does not exist: {args.data_dir}")
        return 1
    
    # Create md_cache directory if it doesn't exist
    if not os.path.exists(args.md_cache):
        os.makedirs(args.md_cache, exist_ok=True)
        print(f"ğŸ“ Created md_cache directory: {args.md_cache}")
    
    try:
        print("ğŸ”§ Initializing embedding model...")
        init_embed_model()
        print("ğŸ”§ Initializing LLM...")
        init_llm()
        
        if args.start_milvus:
            print("ğŸ”§ Starting Milvus server...")
            start_milvus(args.milvus_data)
        
        print("ğŸ”§ Starting data processing...")
        print(f"ğŸ“‚ Data directory: {args.data_dir}")
        print(f"ğŸ“ MD cache directory: {args.md_cache}")
        print(f"ğŸ—„ï¸ Milvus data path: {args.milvus_data}")
        print(f"ğŸ”— Database URI: {args.db_uri}")
        
        index = process(args.data_dir, args.md_cache, args.db_uri)
        
        # ç¡®ä¿èµ„æºæ­£ç¡®æ¸…ç†
        if index is not None:
            if hasattr(index, 'storage_context'):
                # å¯é€‰ï¼šæŒä¹…åŒ–ä¿å­˜
                # index.storage_context.persist()
                
                # æ˜¾å¼å…³é—­å­˜å‚¨ä¸Šä¸‹æ–‡
                if hasattr(index.storage_context, 'vector_store'):
                    vector_store = index.storage_context.vector_store
                    if hasattr(vector_store, 'client'):
                        vector_store.client.close()
            
            # æ¸…ç†ç´¢å¼•
            del index
            
        return 0
            
    except Exception as e:
        print(f"ğŸ”´ Error occurred during execution: {str(e)}")
        traceback.print_exc()
        return 1
    finally:
        if milvus_server is not None:
            try:
                milvus_server.stop()
                print("âœ… Milvus server stopped")
            except Exception as e:
                print(f"âš ï¸ Error occurred while stopping Milvus server: {e}")
        print("Program execution completed")


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
