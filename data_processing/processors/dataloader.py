import os
import sys
import tempfile
import json
import time
import shutil
import traceback
import html2text
import argparse
from dotenv import load_dotenv
from llama_index.core import SimpleDirectoryReader, Document
from llama_index.core import Settings
from llama_index.core.readers.base import BaseReader
from llama_index.core.node_parser import HTMLNodeParser, SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.extractors import TitleExtractor, KeywordExtractor
from llama_index.core.schema import BaseNode, TextNode
from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader, 
    StorageContext,
    Document
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.vector_stores.milvus import MilvusVectorStore
# from sentence_transformers import SentenceTransformer
from bs4 import BeautifulSoup
from pymilvus import MilvusClient
from huggingface_hub import snapshot_download
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from milvus_lite.server import Server
import data_cleaner
from code_extractor import CodeExtractor
from bge_onnx_llama_wrapper import BGEOpenXEmbedding


if os.path.exists('../../.env'):
    load_dotenv('../../.env')

# Default values
DEFAULT_DB_URI = 'http://localhost:19530'
DEFAULT_DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', '..', 'zh-cn-orig', 'docs')
DEFAULT_MILVUS_DATA = "milvus_data"
DEFAULT_START_MILVUS = False

milvus_server = None


def init_embed_model():
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–è®¾ç½®
    hf_endpoint = os.getenv("HF_ENDPOINT", "https://hf-mirror.com")
    hf_base_url = os.getenv("HUGGINGFACE_HUB_BASE_URL", "https://hf-mirror.com")
    model_name = os.getenv("EMBEDDING_MODEL", "BAAI/bge-small-zh-v1.5")
    device = os.getenv("EMBEDDING_DEVICE", "cpu")
    backend = os.getenv("EMBEDDING_BACKEND", "torch")  # torch, onnx, openvino
    cache_dir = os.getenv("EMBEDDING_CACHE_DIR", "hf_cache")
    cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'hf_cache')
    
    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    model_path = model_name
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æ¨¡å‹ç›®å½•
    local_dir = os.getenv("EMBEDDING_LOCAL_DIR", "").strip()
    if local_dir and os.path.isdir(local_dir):
        model_path = local_dir
        print(f"Using local model: {model_path}")
    else:
        # å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œå°è¯•HuggingFaceçš„ç¼“å­˜æˆ–ä¸‹è½½
        if not os.path.exists(cache_dir):
            os.mkdir(cache_dir)
        print(f"Trying to download {model_name} to {cache_dir} from {hf_endpoint}")
        model_path = snapshot_download(
            model_name,
            endpoint=hf_endpoint,
            cache_dir=cache_dir
        )
        print(f"model downloaded to {model_path}")
    
    print(f"Create embedding model")
    print(f"model_path: {model_path}")
    print(f"device: {device}")
    print(f"backend: {backend}")
    
    # æ ¹æ®åç«¯ç±»å‹åˆå§‹åŒ–æ¨¡å‹
    if backend == "onnx":
        # æ£€æŸ¥ONNXæ¨¡å‹è·¯å¾„
        onnx_path = os.path.join(model_path, "onnx", "model.onnx")
        if not os.path.exists(onnx_path):
            print(f"ONNX model path not found: {onnx_path}")
            print("Falling back to PyTorch backend")
            # å›é€€åˆ°PyTorchåç«¯
            Settings.embed_model = HuggingFaceEmbedding(
                model_name=model_path,
                device=device,
                backend="torch"
            )
        else:
            # ä½¿ç”¨æˆ‘ä»¬çš„BGE ONNXåŒ…è£…å™¨
            print("Using BGE ONNX embedding model")
            Settings.embed_model = BGEOpenXEmbedding(
                model_path=model_path,
                device=device,
                cache_dir=cache_dir,
                batch_size=128
            )
    else:
        # PyTorchåç«¯ï¼ˆé»˜è®¤ï¼‰
        Settings.embed_model = HuggingFaceEmbedding(
            model_name=model_path,
            device=device,
            backend="torch"
        )


def init_llm():
    # ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®è‡ªå®šä¹‰ç½‘å…³
    base_url = os.getenv("OPENAI_BASE_URL") or os.getenv("LLM_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
    model_name = os.getenv("LLM_MODEL", "qwen-plus")
    Settings.llm = OpenAI(model=model_name, base_url=base_url, api_key=api_key)


def start_milvus(milvus_data_path=DEFAULT_MILVUS_DATA, port=19530):
    global milvus_server
    try:
        print("ğŸš€ Starting Milvus server...")
        print(f"ğŸ“ Milvus data path: {milvus_data_path}")
        milvus_server = Server(db_file=milvus_data_path, address=f'localhost:{port}')
        milvus_server.start()
        print("âœ… Milvus server started successfully")
        
        # ç­‰å¾…æœåŠ¡å™¨å®Œå…¨å¯åŠ¨
        import time
        print("â³ Waiting for server to fully start...")
        time.sleep(3)
        
        # æµ‹è¯•è¿æ¥
        try:
            from pymilvus import connections
            connections.connect("default", host="localhost", port=port)
            print("âœ… Milvus connection test successful")
            connections.disconnect("default")
        except Exception as e:
            print(f"âš ï¸ Milvus connection test failed: {e}")
    except Exception as e:
        print(f"ğŸ”´ Milvus server startup failed: {e}")
        milvus_server = None
        raise


def init_vector_store(db_uri):
    # åˆ›å»º MilvusVectorStore å®ä¾‹
    # åŠ¨æ€æ¨æ–­å‘é‡ç»´åº¦ä»¥åŒ¹é…å½“å‰åµŒå…¥æ¨¡å‹
    try:
        inferred_dim = len(Settings.embed_model.get_text_embedding("__dim_probe__"))
    except Exception:
        inferred_dim = 512  # BGE-small-zh-v1.5 512ç»´ç‰ˆæœ¬

    vector_store = MilvusVectorStore(
        uri=db_uri,                        # è¿æ¥åœ°å€
        dim=inferred_dim,                  # å‘é‡ç»´åº¦
        collection_name=os.environ['COLLECTION_NAME'],   # é›†åˆåç§°
        # enable_sparse=True,  # å¯ç”¨ç¨€ç–å‘é‡ï¼ˆBM25ï¼‰
        overwrite=True,                    # å¦‚æœé›†åˆå­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–
        consistency_level="Strong"         # ä¸€è‡´æ€§çº§åˆ«ï¼šStrong, Session, Bounded, Eventually
    )

    print("âœ… MilvusVectorStore initialized successfully!")
    print(f"ğŸ“Š Collection name: {vector_store.collection_name}")
    print(f"ğŸ”— Connection URI: {db_uri}")
    print(f"ğŸ“ Vector dimension: {inferred_dim}")
    return vector_store


def process_with_html_parser(data_dir, db_uri):
    # Initialize
    vector_store = init_vector_store(db_uri)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # ä»ç¯å¢ƒå˜é‡è¯»å–chunké…ç½®
    chunk_size = int(os.getenv("CHUNK_SIZE", "1024"))  # é»˜è®¤1024ï¼Œæ¯”512å¤§ä¸€äº›
    chunk_overlap = int(os.getenv("CHUNK_OVERLAP", "100"))  # é»˜è®¤100

    print(f"ğŸ“ ä½¿ç”¨chunk_size: {chunk_size}, chunk_overlap: {chunk_overlap}")

    # Define transformations
    # ä½¿ç”¨SentenceSplitterï¼Œåœ¨æ–‡æ¡£é¢„å¤„ç†é˜¶æ®µå·²ç»æ¸…ç†äº†HTML
    transformations = [
        # ç›´æ¥ä½¿ç”¨SentenceSplitterï¼ŒæŒ‰chunk_sizeåˆ‡åˆ†
        SentenceSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separator="\n\n"  # ä½¿ç”¨åŒæ¢è¡Œç¬¦ä½œä¸ºåˆ†éš”ç¬¦
        ),
        # These extractors depends on LLM, so we disable them
        # TitleExtractor(),
        # KeywordExtractor(keywords=3),
    ]
    
    # Create ingestion pipeline
    pipeline = IngestionPipeline(transformations=transformations)

    print(f"Going to load data from {data_dir}")
    # ä½¿ç”¨æ›´çµæ´»çš„è¿‡æ»¤æ–¹å¼ï¼Œè¿‡æ»¤æ‰è·¯å¾„ä¸­åŒ…å« _print/index.html çš„æ–‡ä»¶
    exclude_files = []
    reader = SimpleDirectoryReader(
        input_dir=data_dir, 
        required_exts=[".html"],
        exclude=exclude_files,
        recursive=True
    )

    batch_nodes = []
    processed_files = 0
    for docs in reader.iter_data():
        for doc in docs:
            # è¿‡æ»¤æ‰è·¯å¾„ä¸­åŒ…å« _print/index.html çš„æ–‡ä»¶
            if doc.metadata and 'file_path' in doc.metadata:
                file_path = doc.metadata['file_path']
                if '_print' in file_path:
                    print(f"ğŸš« Skip file: {file_path} (including _print)")
                    continue
                print(f"Processing file: {file_path}")
            
            # processed_doc = preprocess_html_document(doc)
            processed_doc = preprocess_html_document_safe(doc)
            nodes = pipeline.run(documents=[processed_doc])
            batch_nodes.extend(nodes)
            processed_files += 1
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºå‰å‡ ä¸ªèŠ‚ç‚¹çš„é•¿åº¦
            if processed_files <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶çš„è°ƒè¯•ä¿¡æ¯
                print(f"ğŸ“Š File {processed_files} is processed into {len(nodes)} ä¸ªèŠ‚ç‚¹")
                for i, node in enumerate(nodes[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªèŠ‚ç‚¹
                    text_len = len(node.text.strip())
                    print(f"  node {i+1}: length={text_len}, preview: {node.text.strip()[:100]}...")
                if len(nodes) > 5:
                    print(f"  ... There are {len(nodes)-5} more nodes")

    print(f"Process {processed_files} files, cleaning and validating {len(batch_nodes)} nodes...")
    # Clean and validate nodes before creating index
    valid_nodes = data_cleaner.clean_and_validate_nodes(batch_nodes, min_text_length=10)
    
    if not valid_nodes:
        raise ValueError("No valid nodes found for index creation")
    
    # Create index from the processed nodes
    print(f"\nğŸ“¥ Starting to build vector index from {len(valid_nodes)} nodes...")
    index = VectorStoreIndex(
        nodes=valid_nodes,
        storage_context=storage_context
    )

    print("âœ… Vector index construction completed!")
    print(f"ğŸ“– Indexed {len(batch_nodes)} nodes")
    return index


def preprocess_html_document_safe(doc: Document) -> Document:
    """
    å®‰å…¨åœ°é¢„å¤„ç†HTMLæ–‡æ¡£ï¼Œæå–ä»£ç å—åˆ°metadataä¸­
    
    Args:
        doc: åŸå§‹æ–‡æ¡£å¯¹è±¡
        
    Returns:
        Document: é¢„å¤„ç†åçš„æ–‡æ¡£å¯¹è±¡
    """
    try:
        # ç¡®ä¿æ–‡æ¡£æ–‡æœ¬æ˜¯å­—ç¬¦ä¸²
        if not isinstance(doc.text, str):
            doc.text = str(doc.text)
        
        # æ¸…ç†HTMLå†…å®¹
        cleaned_html = data_cleaner.clean_html_content(doc.text)
        
        # æå–ä»£ç å—
        code_extractor = CodeExtractor()
        processed_html, extracted_codes = code_extractor.extract_code_blocks(cleaned_html)
        
        # é‡æ–°è§£æå¤„ç†åçš„HTML
        processed_soup = BeautifulSoup(processed_html, "html.parser")
        
        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        content_div = processed_soup.find("div", class_="td-content")
        if content_div:
            # æå–çº¯æ–‡æœ¬ï¼Œç§»é™¤HTMLæ ‡ç­¾
            processed_text = content_div.get_text(separator="\n", strip=True)
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šåŒºåŸŸï¼Œæå–æ•´ä¸ªæ–‡æ¡£çš„çº¯æ–‡æœ¬
            processed_text = processed_soup.get_text(separator="\n", strip=True)
        
        # æ¸…ç†å¤„ç†åçš„æ–‡æœ¬
        processed_text = data_cleaner.clean_text(processed_text)
        
        # æ¸…ç†å…ƒæ•°æ®
        cleaned_metadata = data_cleaner.clean_metadata(doc.metadata)
        
        # å°†æå–çš„ä»£ç å—æ·»åŠ åˆ°å…ƒæ•°æ®ä¸­
        if extracted_codes:
            cleaned_metadata['extracted_codes'] = extracted_codes
            cleaned_metadata['code_blocks_count'] = len(extracted_codes)
            print(f"Extracted {len(extracted_codes)} code blocks")
        
        # åˆ›å»ºæ–°çš„æ–‡æ¡£å¯¹è±¡
        return Document(text=processed_text, metadata=cleaned_metadata)
        
    except Exception as e:
        print(f"Exception occurs when preprocessing HTML document: {e}")
        # å¦‚æœå‡ºé”™ï¼Œè¿”å›æ¸…ç†åçš„åŸå§‹æ–‡æ¡£
        return data_cleaner.clean_document(doc)


def main():
    """Main function to handle command line arguments and execute data processing"""
    parser = argparse.ArgumentParser(description='Process HTML documents and create vector index')
    parser.add_argument('--data-dir', type=str, default=DEFAULT_DATA_DIR,
                        help=f'Path to the data directory containing HTML files (default: {DEFAULT_DATA_DIR})')
    parser.add_argument('--start-milvus', action='store_true', default=DEFAULT_START_MILVUS,
                        help='Whether to start Milvus server (default: False)')
    parser.add_argument('--milvus-data', type=str, default=DEFAULT_MILVUS_DATA,
                        help=f'Path to Milvus database files (default: {DEFAULT_MILVUS_DATA})')
    parser.add_argument('--db-uri', type=str, default=DEFAULT_DB_URI,
                        help=f'Milvus database URI (default: {DEFAULT_DB_URI})')
    
    args = parser.parse_args()
    
    # Validate paths
    if not os.path.exists(args.data_dir):
        print(f"ğŸ”´ Error: Data directory does not exist: {args.data_dir}")
        return 1
    
    try:
        print("ğŸ”§ Initializing embedding model...")
        init_embed_model()
        print("ğŸ”§ Initializing LLM...")
        init_llm()
        
        if args.start_milvus:
            print("ğŸ”§ Starting Milvus server...")
            start_milvus(args.milvus_data)
        
        print("ğŸ”§ Starting data processing...")
        print(f"ğŸ“‚ Data directory: {args.data_dir}")
        print(f"ğŸ”— Database URI: {args.db_uri}")
        
        index = process_with_html_parser(args.data_dir, args.db_uri)
        
        # ç¡®ä¿èµ„æºæ­£ç¡®æ¸…ç†
        if index is not None:
            if hasattr(index, 'storage_context'):
                # å¯é€‰ï¼šæŒä¹…åŒ–ä¿å­˜
                # index.storage_context.persist()
                
                # æ˜¾å¼å…³é—­å­˜å‚¨ä¸Šä¸‹æ–‡
                if hasattr(index.storage_context, 'vector_store'):
                    vector_store = index.storage_context.vector_store
                    if hasattr(vector_store, 'client'):
                        vector_store.client.close()
            
            # æ¸…ç†ç´¢å¼•
            del index
            
        return 0
            
    except Exception as e:
        print(f"ğŸ”´ Error occurred during execution: {str(e)}")
        traceback.print_exc()
        return 1
    finally:
        if milvus_server is not None:
            try:
                milvus_server.stop()
                print("âœ… Milvus server stopped")
            except Exception as e:
                print(f"âš ï¸ Error occurred while stopping Milvus server: {e}")
        print("Program execution completed")


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
