"""
æ–‡æ¡£å¤„ç†å™¨ - æ–‡æœ¬åˆ†å‰²å’Œå‘é‡åŒ–
Uses shared modules for embedding and configuration

@deprecated: æ­¤æ¨¡å—å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨æ–°çš„æµæ°´çº¿æ¶æ„

æ–°çš„ä½¿ç”¨æ–¹å¼:
    from data_processing.processors import PipelineRunner
    
    runner = PipelineRunner()
    result = await runner.run(data_dir="./data/zh-cn", storage_backend="milvus")

æˆ–ä½¿ç”¨å‘½ä»¤è¡Œ:
    python -m data_processing.processors.cli --data-dir ./data/zh-cn --backend milvus
"""

import warnings
warnings.warn(
    "document_processor.py is deprecated. Use 'from data_processing.processors import PipelineRunner' instead.",
    DeprecationWarning,
    stacklevel=2
)

import os
import sys
import uuid
import asyncio
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

# Add project root to path for shared module imports
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from dotenv import load_dotenv

# Load environment variables
env_path = os.path.join(project_root, ".env")
if os.path.exists(env_path):
    load_dotenv(env_path)

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# Import from shared modules
from shared.config import get_settings
from shared.embeddings import create_embedding_service, EmbeddingService


# Simple logger for data processing
import logging

def get_logger(name: str) -> logging.Logger:
    """Get a configured logger"""
    logger = logging.getLogger(name)
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        ))
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger


logger = get_logger("DocumentProcessor")


class MilvusServiceWrapper:
    """
    Wrapper for Milvus operations
    Uses pymilvus directly instead of backend service
    """
    
    def __init__(self, uri: str, collection_name: str, vector_dim: int):
        self.uri = uri
        self.collection_name = collection_name
        self.vector_dim = vector_dim
        self.client = None
        self.logger = get_logger("MilvusServiceWrapper")
    
    async def initialize(self):
        """Initialize Milvus connection"""
        from pymilvus import MilvusClient, connections, Collection, CollectionSchema, FieldSchema, DataType
        from urllib.parse import urlparse
        
        try:
            # Parse URI
            raw_uri = self.uri.strip()
            if "://" in raw_uri:
                parsed = urlparse(raw_uri)
                host = parsed.hostname
                port = parsed.port
                client_uri = f"{parsed.scheme}://{host}:{port}"
            else:
                if ":" not in raw_uri:
                    raise ValueError(f"Invalid MILVUS_URI (missing port): {raw_uri}")
                host, port_str = raw_uri.rsplit(":", 1)
                port = int(port_str)
                client_uri = f"http://{host}:{port}"
            
            # Connect to Milvus
            self.logger.info(f"Connecting to Milvus: {host}:{port}")
            connections.connect(alias="default", host=host, port=port)
            
            # Create MilvusClient
            self.client = MilvusClient(uri=client_uri, token="")
            
            # Ensure collection exists
            await self._ensure_collection_exists()
            
            self.logger.info(f"âœ… Milvus connection initialized: {self.uri}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize Milvus: {e}")
            raise
    
    async def _ensure_collection_exists(self):
        """Ensure collection exists"""
        from pymilvus import Collection, CollectionSchema, FieldSchema, DataType
        
        collections = self.client.list_collections()
        
        if self.collection_name not in collections:
            # Create collection
            fields = [
                FieldSchema(name="id", dtype=DataType.VARCHAR, max_length=65535, is_primary=True),
                FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="metadata", dtype=DataType.JSON),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.vector_dim)
            ]
            
            schema = CollectionSchema(
                fields=fields,
                description="K8s æ–‡æ¡£å‘é‡å­˜å‚¨"
            )
            
            self.client.create_collection(
                collection_name=self.collection_name,
                schema=schema
            )
            
            # Create index
            collection = Collection(self.collection_name)
            collection.create_index(
                field_name="embedding",
                index_params={
                    "index_type": "IVF_FLAT",
                    "metric_type": "COSINE",
                    "params": {"nlist": 1024}
                }
            )
            collection.load()
            
            self.logger.info(f"âœ… Created collection: {self.collection_name}")
        else:
            collection = Collection(self.collection_name)
            collection.load()
            self.logger.info(f"âœ… Collection already exists: {self.collection_name}")
    
    async def insert_documents(self, documents: List[Dict[str, Any]]):
        """Insert documents into vector database"""
        if not documents:
            self.logger.warning("âš ï¸ No documents to insert")
            return
        
        data = []
        for doc in documents:
            if not doc.get("id") or not doc.get("content") or not doc.get("embedding"):
                continue
            data.append({
                "id": doc["id"],
                "content": doc["content"],
                "metadata": doc.get("metadata", {}),
                "embedding": doc["embedding"]
            })
        
        if data:
            self.client.insert(
                collection_name=self.collection_name,
                data=data
            )
            self.logger.info(f"âœ… Successfully inserted {len(data)} documents")
    
    async def close(self):
        """Close connection"""
        from pymilvus import connections
        try:
            if self.client:
                self.client.close()
            connections.disconnect("default")
            self.logger.info("âœ… Milvus connection closed")
        except Exception as e:
            self.logger.error(f"âŒ Failed to close Milvus connection: {e}")


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ - ä½¿ç”¨sharedæ¨¡å—"""
    
    def __init__(
        self,
        milvus_uri: str = None,
        collection_name: str = None,
        chunk_size: int = None,
        chunk_overlap: int = None
    ):
        # Get settings
        settings = get_settings()
        
        # Use provided values or defaults from settings
        self.milvus_uri = milvus_uri or os.getenv("MILVUS_URI", settings.MILVUS_URI)
        self.collection_name = collection_name or os.getenv("COLLECTION_NAME", settings.COLLECTION_NAME)
        self.chunk_size = chunk_size or int(os.getenv("CHUNK_SIZE", str(settings.CHUNK_SIZE)))
        self.chunk_overlap = chunk_overlap or int(os.getenv("CHUNK_OVERLAP", str(settings.CHUNK_OVERLAP)))
        
        # Initialize embedding service using shared module
        self.embedding_service = create_embedding_service(use_singleton=True)
        
        # Get vector dimension from embedding service
        self.vector_dim = self.embedding_service.get_embedding_dimension()
        
        # Initialize Milvus wrapper
        self.milvus_service = MilvusServiceWrapper(
            uri=self.milvus_uri,
            collection_name=self.collection_name,
            vector_dim=self.vector_dim
        )
        
        # Create text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
        print(f'ğŸ”§ å½“å‰é…ç½®:')
        print(f'   - MILVUS_URI: {self.milvus_uri}')
        print(f'   - COLLECTION_NAME: {self.collection_name}')
        print(f'   - CHUNK_SIZE: {self.chunk_size}')
        print(f'   - CHUNK_OVERLAP: {self.chunk_overlap}')
        print(f'   - VECTOR_DIM: {self.vector_dim}')
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        await self.milvus_service.initialize()
        logger.info("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def process_documents(self, docs_dir: str = None):
        """å¤„ç†æ–‡æ¡£ç›®å½•"""
        docs_path = Path(docs_dir or "docs")
        if not docs_path.exists():
            logger.error(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
            return
        
        logger.info(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡æ¡£ç›®å½•: {docs_dir}")
        
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
        doc_files = []
        for ext in ['.txt', '.md', '.html']:
            doc_files.extend(docs_path.rglob(f"*{ext}"))
        
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£
        all_chunks = []
        for doc_file in doc_files:
            try:
                chunks = await self._process_single_document(doc_file)
                all_chunks.extend(chunks)
                logger.info(f"âœ… å¤„ç†æ–‡æ¡£: {doc_file.name} -> {len(chunks)} ä¸ªå—")
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡æ¡£å¤±è´¥ {doc_file.name}: {e}")
        
        logger.info(f"ğŸ“Š æ€»å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # æ‰¹é‡å‘é‡åŒ–
        if all_chunks:
            await self._vectorize_and_store(all_chunks)
        
        logger.info("âœ… æ–‡æ¡£å¤„ç†å®Œæˆ")
    
    async def _process_single_document(self, file_path: Path) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await self._read_file(file_path)
        if not content:
            return []
        
        # æå–å…ƒæ•°æ®
        metadata = self._extract_metadata(file_path, content)
        
        # æ–‡æœ¬åˆ†å‰²
        chunks = self._split_text(content, metadata)
        
        return chunks
    
    async def _read_file(self, file_path: Path) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def _extract_metadata(self, file_path: Path, content: str) -> Dict[str, Any]:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        metadata = {
            "filename": file_path.name,
            "file_path": str(file_path),
            "file_size": len(content),
            "file_type": file_path.suffix,
            "title": file_path.stem
        }
        
        # å°è¯•ä»å†…å®¹ä¸­æå–æ ‡é¢˜
        lines = content.split('\n')
        for line in lines[:10]:
            line = line.strip()
            if line.startswith('# '):
                metadata["title"] = line[2:].strip()
                break
            elif line.startswith('title:'):
                metadata["title"] = line[6:].strip()
                break
        
        return metadata
    
    def _split_text(self, content: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†å‰²æ–‡æœ¬"""
        # åˆ›å»º LangChain Document
        doc = Document(page_content=content, metadata=metadata)
        
        # åˆ†å‰²æ–‡æœ¬
        chunks = self.text_splitter.split_documents([doc])
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        result = []
        for i, chunk in enumerate(chunks):
            chunk_data = {
                "id": str(uuid.uuid4()),
                "content": chunk.page_content,
                "metadata": {
                    **chunk.metadata,
                    "chunk_index": i,
                    "chunk_id": str(uuid.uuid4())
                }
            }
            result.append(chunk_data)
        
        return result
    
    async def _vectorize_and_store(self, chunks: List[Dict[str, Any]]):
        """å‘é‡åŒ–å¹¶å­˜å‚¨æ–‡æœ¬å—"""
        logger.info("ğŸ”„ å¼€å§‹å‘é‡åŒ–æ–‡æœ¬å—...")
        
        # æ‰¹é‡ç¼–ç  using shared embedding service
        texts = [chunk["content"] for chunk in chunks]
        embeddings = self.embedding_service.encode_batch(texts, batch_size=32)
        
        # å‡†å¤‡å­˜å‚¨æ•°æ®
        documents = []
        for chunk, embedding in zip(chunks, embeddings):
            document = {
                "id": chunk["id"],
                "content": chunk["content"],
                "metadata": chunk["metadata"],
                "embedding": embedding
            }
            documents.append(document)
        
        # å­˜å‚¨åˆ° Milvus
        await self.milvus_service.insert_documents(documents)
        
        logger.info(f"âœ… æˆåŠŸå­˜å‚¨ {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
    
    async def process_single_file(self, file_path: str) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            chunks = await self._process_single_document(file_path)
            if chunks:
                await self._vectorize_and_store(chunks)
                logger.info(f"âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ: {file_path.name}")
                return True
            else:
                logger.warning(f"âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©º: {file_path.name}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False
    
    async def close(self):
        """å…³é—­æœåŠ¡"""
        await self.milvus_service.close()


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="K8s Assistant æ–‡æ¡£å¤„ç†å™¨")
    parser.add_argument(
        "--milvus-uri", 
        default="http://localhost:19530",
        help="Milvus æœåŠ¡åœ°å€ (é»˜è®¤: http://localhost:19530)"
    )
    parser.add_argument(
        "--collection-name", 
        default="k8s_docs",
        help="é›†åˆåç§° (é»˜è®¤: k8s_docs)"
    )
    parser.add_argument(
        "--docs-dir", 
        default="docs",
        help="æ–‡æ¡£ç›®å½•è·¯å¾„ (é»˜è®¤: docs)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=512,
        help="æ–‡æœ¬å—å¤§å° (é»˜è®¤: 512)"
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=50,
        help="æ–‡æœ¬å—é‡å å¤§å° (é»˜è®¤: 50)"
    )
    parser.add_argument(
        "--single-file",
        help="å¤„ç†å•ä¸ªæ–‡ä»¶"
    )
    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    processor = DocumentProcessor(
        milvus_uri=args.milvus_uri,
        collection_name=args.collection_name,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap
    )
    
    try:
        await processor.initialize()
        
        if args.single_file:
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            success = await processor.process_single_file(args.single_file)
            if success:
                logger.info("âœ… å•æ–‡ä»¶å¤„ç†å®Œæˆ")
            else:
                logger.error("âŒ å•æ–‡ä»¶å¤„ç†å¤±è´¥")
        else:
            # å¤„ç†æ–‡æ¡£ç›®å½•
            await processor.process_documents(args.docs_dir)
            
    finally:
        await processor.close()


if __name__ == "__main__":
    asyncio.run(main())
