"""
æ–‡æ¡£å¤„ç†å™¨ - æ–‡æœ¬åˆ†å‰²å’Œå‘é‡åŒ–
"""

import os
import sys
import uuid
import asyncio
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

# åœ¨å¯¼å…¥ä»»ä½•æ¨¡å—ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
def set_environment_variables(milvus_uri: str = None, collection_name: str = None, chunk_size: int = None, chunk_overlap: int = None):
    """è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¿…é¡»åœ¨å¯¼å…¥ä»»ä½•æ¨¡å—ä¹‹å‰è°ƒç”¨"""
    if milvus_uri:
        os.environ["MILVUS_URI"] = milvus_uri
        print(f'âœ… è®¾ç½® MILVUS_URI: {milvus_uri}')
    if collection_name:
        os.environ["COLLECTION_NAME"] = collection_name
        print(f'âœ… è®¾ç½® COLLECTION_NAME: {collection_name}')
    if chunk_size:
        os.environ["CHUNK_SIZE"] = str(chunk_size)
        print(f'âœ… è®¾ç½® CHUNK_SIZE: {chunk_size}')
    if chunk_overlap:
        os.environ["CHUNK_OVERLAP"] = str(chunk_overlap)
        print(f'âœ… è®¾ç½® CHUNK_OVERLAP: {chunk_overlap}')

# è§£æå‘½ä»¤è¡Œå‚æ•°
def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="K8s Assistant æ–‡æ¡£å¤„ç†å™¨")
    parser.add_argument(
        "--milvus-uri", 
        default="http://localhost:19530",
        help="Milvus æœåŠ¡åœ°å€ (é»˜è®¤: http://localhost:19530)"
    )
    parser.add_argument(
        "--collection-name", 
        default="k8s_docs",
        help="é›†åˆåç§° (é»˜è®¤: k8s_docs)"
    )
    parser.add_argument(
        "--docs-dir", 
        default="docs",
        help="æ–‡æ¡£ç›®å½•è·¯å¾„ (é»˜è®¤: docs)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=512,
        help="æ–‡æœ¬å—å¤§å° (é»˜è®¤: 512)"
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=50,
        help="æ–‡æœ¬å—é‡å å¤§å° (é»˜è®¤: 50)"
    )
    parser.add_argument(
        "--single-file",
        help="å¤„ç†å•ä¸ªæ–‡ä»¶"
    )
    return parser.parse_args()

# è·å–å‘½ä»¤è¡Œå‚æ•°å¹¶è®¾ç½®ç¯å¢ƒå˜é‡
args = parse_arguments()
set_environment_variables(
    milvus_uri=args.milvus_uri,
    collection_name=args.collection_name,
    chunk_size=args.chunk_size,
    chunk_overlap=args.chunk_overlap
)

# ç°åœ¨å¯¼å…¥é¡¹ç›®æ¨¡å—ï¼ˆç¯å¢ƒå˜é‡å·²ç»è®¾ç½®ï¼‰
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'backend'))

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from app.services.embedding_service import EmbeddingService
from app.services.milvus_service import MilvusService
from app.core.config import settings
from app.core.logging import get_logger

logger = get_logger("DocumentProcessor")


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.milvus_service = MilvusService()
        
        # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
        print(f'ğŸ”§ å½“å‰é…ç½®:')
        print(f'   - MILVUS_URI: {settings.MILVUS_URI}')
        print(f'   - COLLECTION_NAME: {settings.COLLECTION_NAME}')
        print(f'   - CHUNK_SIZE: {settings.CHUNK_SIZE}')
        print(f'   - CHUNK_OVERLAP: {settings.CHUNK_OVERLAP}')
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        await self.milvus_service.initialize()
        logger.info("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def process_documents(self, docs_dir: str = None):
        """å¤„ç†æ–‡æ¡£ç›®å½•"""
        docs_path = Path(docs_dir or "docs")
        if not docs_path.exists():
            logger.error(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
            return
        
        logger.info(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡æ¡£ç›®å½•: {docs_dir}")
        
        # æ”¶é›†æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
        doc_files = []
        for ext in ['.txt', '.md', '.html']:
            doc_files.extend(docs_path.rglob(f"*{ext}"))
        
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£
        all_chunks = []
        for doc_file in doc_files:
            try:
                chunks = await self._process_single_document(doc_file)
                all_chunks.extend(chunks)
                logger.info(f"âœ… å¤„ç†æ–‡æ¡£: {doc_file.name} -> {len(chunks)} ä¸ªå—")
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡æ¡£å¤±è´¥ {doc_file.name}: {e}")
        
        logger.info(f"ğŸ“Š æ€»å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # æ‰¹é‡å‘é‡åŒ–
        if all_chunks:
            await self._vectorize_and_store(all_chunks)
        
        logger.info("âœ… æ–‡æ¡£å¤„ç†å®Œæˆ")
    
    async def _process_single_document(self, file_path: Path) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await self._read_file(file_path)
        if not content:
            return []
        
        # æå–å…ƒæ•°æ®
        metadata = self._extract_metadata(file_path, content)
        
        # æ–‡æœ¬åˆ†å‰²
        chunks = self._split_text(content, metadata)
        
        return chunks
    
    async def _read_file(self, file_path: Path) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def _extract_metadata(self, file_path: Path, content: str) -> Dict[str, Any]:
        """æå–æ–‡æ¡£å…ƒæ•°æ®"""
        metadata = {
            "filename": file_path.name,
            "file_path": str(file_path),
            "file_size": len(content),
            "file_type": file_path.suffix,
            "title": file_path.stem
        }
        
        # å°è¯•ä»å†…å®¹ä¸­æå–æ ‡é¢˜
        lines = content.split('\n')
        for line in lines[:10]:  # åªæ£€æŸ¥å‰10è¡Œ
            line = line.strip()
            if line.startswith('# '):
                metadata["title"] = line[2:].strip()
                break
            elif line.startswith('title:'):
                metadata["title"] = line[6:].strip()
                break
        
        return metadata
    
    def _split_text(self, content: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†å‰²æ–‡æœ¬"""
        # åˆ›å»º LangChain Document
        doc = Document(page_content=content, metadata=metadata)
        
        # åˆ†å‰²æ–‡æœ¬
        chunks = self.text_splitter.split_documents([doc])
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        result = []
        for i, chunk in enumerate(chunks):
            chunk_data = {
                "id": str(uuid.uuid4()),
                "content": chunk.page_content,
                "metadata": {
                    **chunk.metadata,
                    "chunk_index": i,
                    "chunk_id": str(uuid.uuid4())
                }
            }
            result.append(chunk_data)
        
        return result
    
    async def _vectorize_and_store(self, chunks: List[Dict[str, Any]]):
        """å‘é‡åŒ–å¹¶å­˜å‚¨æ–‡æœ¬å—"""
        logger.info("ğŸ”„ å¼€å§‹å‘é‡åŒ–æ–‡æœ¬å—...")
        
        # æ‰¹é‡ç¼–ç 
        texts = [chunk["content"] for chunk in chunks]
        embeddings = self.embedding_service.encode_batch(texts, batch_size=32)
        
        # å‡†å¤‡å­˜å‚¨æ•°æ®
        documents = []
        for chunk, embedding in zip(chunks, embeddings):
            document = {
                "id": chunk["id"],
                "content": chunk["content"],
                "metadata": chunk["metadata"],
                "embedding": embedding
            }
            documents.append(document)
        
        # å­˜å‚¨åˆ° Milvus
        await self.milvus_service.insert_documents(documents)
        
        logger.info(f"âœ… æˆåŠŸå­˜å‚¨ {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
    
    async def process_single_file(self, file_path: str) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            chunks = await self._process_single_document(file_path)
            if chunks:
                await self._vectorize_and_store(chunks)
                logger.info(f"âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ: {file_path.name}")
                return True
            else:
                logger.warning(f"âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©º: {file_path.name}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False
    
    async def close(self):
        """å…³é—­æœåŠ¡"""
        await self.milvus_service.close()


async def main():
    """ä¸»å‡½æ•°"""
    processor = DocumentProcessor()
    
    try:
        await processor.initialize()
        
        if args.single_file:
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            success = await processor.process_single_file(args.single_file)
            if success:
                logger.info("âœ… å•æ–‡ä»¶å¤„ç†å®Œæˆ")
            else:
                logger.error("âŒ å•æ–‡ä»¶å¤„ç†å¤±è´¥")
        else:
            # å¤„ç†æ–‡æ¡£ç›®å½•
            await processor.process_documents(args.docs_dir)
            
    finally:
        await processor.close()


if __name__ == "__main__":
    asyncio.run(main())
