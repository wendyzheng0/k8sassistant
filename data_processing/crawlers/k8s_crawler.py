"""
Kubernetes æ–‡æ¡£çˆ¬å–å™¨
"""

import os
import asyncio
import aiohttp
import aiofiles
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Set
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class K8sCrawler:
    """Kubernetes æ–‡æ¡£çˆ¬å–å™¨"""
    
    def __init__(self, output_dir: str = "docs"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Kubernetes å®˜æ–¹æ–‡æ¡£åŸºç¡€URL
        self.base_url = "https://kubernetes.io"
        self.docs_url = "https://kubernetes.io/docs"
        
        # å·²è®¿é—®çš„URLé›†åˆ
        self.visited_urls: Set[str] = set()
        
        # è¦çˆ¬å–çš„é¡µé¢ç±»å‹
        self.target_paths = [
            "/docs/concepts/",
            "/docs/tasks/",
            "/docs/reference/",
            "/docs/setup/",
            "/docs/tutorials/"
        ]
    
    async def crawl(self, max_pages: int = 100):
        """å¼€å§‹çˆ¬å–æ–‡æ¡£"""
        logger.info("ğŸš€ å¼€å§‹çˆ¬å– Kubernetes æ–‡æ¡£...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        for path in self.target_paths:
            (self.output_dir / path.strip("/")).mkdir(parents=True, exist_ok=True)
        
        # æ”¶é›†æ‰€æœ‰è¦çˆ¬å–çš„URL
        urls_to_crawl = []
        
        async with aiohttp.ClientSession() as session:
            for path in self.target_paths:
                full_url = urljoin(self.base_url, path)
                logger.info(f"ğŸ” æ‰«æç›®å½•: {full_url}")
                
                try:
                    urls = await self._get_page_urls(session, full_url)
                    urls_to_crawl.extend(urls)
                except Exception as e:
                    logger.error(f"âŒ æ‰«æç›®å½•å¤±è´¥ {full_url}: {e}")
        
        # é™åˆ¶çˆ¬å–é¡µé¢æ•°é‡
        urls_to_crawl = urls_to_crawl[:max_pages]
        logger.info(f"ğŸ“‹ å‡†å¤‡çˆ¬å– {len(urls_to_crawl)} ä¸ªé¡µé¢")
        
        # å¹¶å‘çˆ¬å–é¡µé¢
        semaphore = asyncio.Semaphore(10)  # é™åˆ¶å¹¶å‘æ•°
        
        tasks = []
        for url in urls_to_crawl:
            task = asyncio.create_task(
                self._crawl_page_with_semaphore(session, url, semaphore)
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r is True)
        logger.info(f"âœ… çˆ¬å–å®Œæˆï¼æˆåŠŸ: {success_count}/{len(urls_to_crawl)}")
    
    async def _crawl_page_with_semaphore(
        self, 
        session: aiohttp.ClientSession, 
        url: str, 
        semaphore: asyncio.Semaphore
    ):
        """ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘çš„é¡µé¢çˆ¬å–"""
        async with semaphore:
            return await self._crawl_page(session, url)
    
    async def _get_page_urls(self, session: aiohttp.ClientSession, url: str) -> List[str]:
        """è·å–é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥"""
        try:
            async with session.get(url) as response:
                if response.status != 200:
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                urls = []
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    
                    # åªå¤„ç†ç›¸å¯¹é“¾æ¥å’ŒåŒåŸŸåçš„ç»å¯¹é“¾æ¥
                    if href.startswith('/'):
                        full_url = urljoin(self.base_url, href)
                    elif href.startswith(self.base_url):
                        full_url = href
                    else:
                        continue
                    
                    # è¿‡æ»¤æ‰éæ–‡æ¡£é¡µé¢
                    if not self._is_valid_doc_url(full_url):
                        continue
                    
                    urls.append(full_url)
                
                return list(set(urls))  # å»é‡
                
        except Exception as e:
            logger.error(f"âŒ è·å–é¡µé¢é“¾æ¥å¤±è´¥ {url}: {e}")
            return []
    
    def _is_valid_doc_url(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ–‡æ¡£URL"""
        if not url.startswith(self.base_url):
            return False
        
        # æ’é™¤ä¸€äº›ä¸éœ€è¦çš„é¡µé¢
        exclude_patterns = [
            '/search',
            '/sitemap',
            '/404',
            '/blog',
            '/community',
            '/partners',
            '/case-studies',
            '#',
            'mailto:',
            'javascript:'
        ]
        
        for pattern in exclude_patterns:
            if pattern in url:
                return False
        
        # åªåŒ…å«æ–‡æ¡£ç›¸å…³çš„è·¯å¾„
        valid_patterns = [
            '/docs/concepts/',
            '/docs/tasks/',
            '/docs/reference/',
            '/docs/setup/',
            '/docs/tutorials/'
        ]
        
        return any(pattern in url for pattern in valid_patterns)
    
    async def _crawl_page(self, session: aiohttp.ClientSession, url: str) -> bool:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        if url in self.visited_urls:
            return False
        
        self.visited_urls.add(url)
        
        try:
            async with session.get(url) as response:
                if response.status != 200:
                    logger.warning(f"âš ï¸ é¡µé¢è®¿é—®å¤±è´¥ {url}: {response.status}")
                    return False
                
                html = await response.text()
                
                # è§£æHTML
                soup = BeautifulSoup(html, 'html.parser')
                
                # æå–ä¸»è¦å†…å®¹
                content = self._extract_content(soup)
                
                if not content.strip():
                    logger.warning(f"âš ï¸ é¡µé¢å†…å®¹ä¸ºç©º {url}")
                    return False
                
                # ç”Ÿæˆæ–‡ä»¶å
                filename = self._generate_filename(url)
                filepath = self.output_dir / filename
                
                # ä¿å­˜æ–‡ä»¶
                async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                    await f.write(f"# {soup.title.string if soup.title else 'Untitled'}\n\n")
                    await f.write(f"URL: {url}\n\n")
                    await f.write(content)
                
                logger.info(f"âœ… ä¿å­˜é¡µé¢: {filename}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–é¡µé¢å¤±è´¥ {url}: {e}")
            return False
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """æå–é¡µé¢ä¸»è¦å†…å®¹"""
        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        content_selectors = [
            'main',
            '.td-content',
            '.content',
            '#content',
            'article',
            '.main-content'
        ]
        
        content_element = None
        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element:
                break
        
        if not content_element:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨body
            content_element = soup.body
        
        if not content_element:
            return ""
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in content_element.find_all(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
        
        # æå–æ–‡æœ¬å†…å®¹
        text = content_element.get_text(separator='\n', strip=True)
        
        # æ¸…ç†æ–‡æœ¬
        lines = []
        for line in text.split('\n'):
            line = line.strip()
            if line and len(line) > 10:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¡Œ
                lines.append(line)
        
        return '\n\n'.join(lines)
    
    def _generate_filename(self, url: str) -> str:
        """æ ¹æ®URLç”Ÿæˆæ–‡ä»¶å"""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        if not path:
            path = 'index'
        
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        filename = path.replace('/', '_').replace('-', '_')
        
        # æ·»åŠ æ‰©å±•å
        if not filename.endswith('.html'):
            filename += '.html'
        
        return filename


async def main():
    """ä¸»å‡½æ•°"""
    crawler = K8sCrawler()
    await crawler.crawl(max_pages=50)  # é™åˆ¶çˆ¬å–50ä¸ªé¡µé¢


if __name__ == "__main__":
    asyncio.run(main())
