"""
å¤æ‚æ£€ç´¢æœåŠ¡ - ç»“åˆ Milvus å‘é‡æ•°æ®åº“ã€Elasticsearch å’Œ CrossEncoder é‡æ’åº

æ··åˆæ£€ç´¢é€»è¾‘:
1. Milvus: å‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
2. Elasticsearch: å…³é”®å­—æ£€ç´¢ï¼ˆBM25ï¼‰
3. RRF: èåˆä¸¤è·¯æ£€ç´¢ç»“æœ
4. CrossEncoder: ç²¾æ’é‡æ’åº
"""

import asyncio
import time
import os
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from sentence_transformers import CrossEncoder
from huggingface_hub import snapshot_download
import torch

from shared.data_access import ElasticsearchClient, ElasticsearchConfig

from app.core.config import settings
from app.core.logging import get_logger
from app.services.milvus_service import MilvusService
from app.services.embedding_service import EmbeddingService


@dataclass
class RetrievalRequest:
    """æ£€ç´¢è¯·æ±‚"""
    query: str
    top_k: int = 10
    milvus_weight: float = 0.6
    elasticsearch_weight: float = 0.4
    rerank_top_k: int = 20
    use_reranking: bool = True


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    query: str
    documents: List[Dict[str, Any]]
    milvus_results: List[Dict[str, Any]]
    elasticsearch_results: List[Dict[str, Any]]
    reranked_results: List[Dict[str, Any]]
    execution_time: float
    metadata: Dict[str, Any]


class RRFReranker:
    """RRF (Reciprocal Rank Fusion) é‡æ’åºå™¨"""
    
    def __init__(self, k: Optional[int] = None):
        """
        åˆå§‹åŒ– RRF é‡æ’åºå™¨
        
        Args:
            k: RRF ç®—æ³•ä¸­çš„å¸¸æ•°ï¼Œé€šå¸¸è®¾ç½®ä¸º 60ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä»é…ç½®ä¸­è¯»å–
        """
        self.logger = get_logger("RRFReranker")
        self.k = k if k is not None else getattr(settings, 'RRF_K', 60)
        self.logger.info(f"ğŸ”„ RRF reranker initialized with k={self.k}")
    
    def rerank(
        self, 
        milvus_results: List[Dict[str, Any]], 
        elasticsearch_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ RRF ç®—æ³•é‡æ’åºç»“æœ
        
        åŸºäºç»Ÿä¸€çš„ doc_id (æ ¼å¼: relative_path#chunk_index) è¿›è¡Œèåˆï¼Œ
        ç¡®ä¿ Milvus å’Œ Elasticsearch è¿”å›çš„ç›¸åŒåˆ†å—èƒ½å¤Ÿæ­£ç¡®åŒ¹é…ã€‚
        
        Args:
            milvus_results: Milvus æœç´¢ç»“æœï¼ˆåŒ…å« doc_id å­—æ®µï¼‰
            elasticsearch_results: Elasticsearch æœç´¢ç»“æœï¼ˆåŒ…å« doc_id å­—æ®µï¼‰
            
        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ”„ Starting RRF reranking with k={self.k}")
            
            # åˆ›å»º doc_id åˆ° RRF åˆ†æ•°çš„æ˜ å°„
            # ä½¿ç”¨ doc_id (relative_path#chunk_index) ä½œä¸ºç»Ÿä¸€æ ‡è¯†ç¬¦è¿›è¡Œèåˆ
            doc_scores = {}
            
            # å¤„ç† Milvus ç»“æœ
            for rank, doc in enumerate(milvus_results):
                # ä¼˜å…ˆä½¿ç”¨ doc_id è¿›è¡Œèåˆï¼Œç¡®ä¿ä¸ ES çš„åˆ†å—èƒ½å¤ŸåŒ¹é…
                doc_id = doc.get('doc_id') or doc.get('id')
                if doc_id:
                    if doc_id not in doc_scores:
                        doc_scores[doc_id] = {
                            'id': doc.get('id', doc_id),  # ä¿ç•™åŸå§‹ id
                            'doc_id': doc_id,  # ç»Ÿä¸€çš„ doc_id
                            'content': doc.get('content', ''),
                            'metadata': doc.get('metadata', {}),
                            'file_path': doc.get('file_path', ''),
                            'chunk_index': doc.get('chunk_index', 0),
                            'rrf_score': 0.0,
                            'milvus_rank': rank + 1,
                            'milvus_score': doc.get('score', 0.0),
                            'elasticsearch_rank': None,
                            'elasticsearch_score': None,
                            'sources': []
                        }
                    
                    # è®¡ç®— RRF åˆ†æ•°ï¼š1 / (k + rank)
                    rrf_score = 1.0 / (self.k + rank + 1)
                    doc_scores[doc_id]['rrf_score'] += rrf_score
                    doc_scores[doc_id]['sources'].append('milvus')
            
            # å¤„ç† Elasticsearch ç»“æœ
            for rank, doc in enumerate(elasticsearch_results):
                # ä½¿ç”¨ doc_id è¿›è¡Œèåˆ
                doc_id = doc.get('doc_id') or doc.get('id')
                if doc_id:
                    if doc_id not in doc_scores:
                        doc_scores[doc_id] = {
                            'id': doc.get('id', doc_id),
                            'doc_id': doc_id,
                            'content': doc.get('content', ''),
                            'metadata': {},
                            'file_path': doc.get('file_path', ''),
                            'chunk_index': doc.get('chunk_index', 0),
                            'rrf_score': 0.0,
                            'milvus_rank': None,
                            'milvus_score': None,
                            'elasticsearch_rank': rank + 1,
                            'elasticsearch_score': doc.get('score', 0.0),
                            'sources': []
                        }
                    else:
                        doc_scores[doc_id]['elasticsearch_rank'] = rank + 1
                        doc_scores[doc_id]['elasticsearch_score'] = doc.get('score', 0.0)
                    
                    # è®¡ç®— RRF åˆ†æ•°ï¼š1 / (k + rank)
                    rrf_score = 1.0 / (self.k + rank + 1)
                    doc_scores[doc_id]['rrf_score'] += rrf_score
                    doc_scores[doc_id]['sources'].append('elasticsearch')
                    
                    # æ·»åŠ é«˜äº®ä¿¡æ¯
                    if 'highlights' in doc:
                        doc_scores[doc_id]['highlights'] = doc['highlights']
            
            # æŒ‰ RRF åˆ†æ•°æ’åº
            reranked_results = list(doc_scores.values())
            reranked_results.sort(key=lambda x: x['rrf_score'], reverse=True)
            
            # ç»Ÿè®¡èåˆæ•ˆæœ
            both_sources = sum(1 for r in reranked_results if len(r.get('sources', [])) == 2)
            self.logger.info(
                f"âœ… RRF reranking completed: {len(reranked_results)} unique docs, "
                f"{both_sources} matched in both sources"
            )
            return reranked_results
            
        except Exception as e:
            self.logger.error(f"âŒ RRF reranking failed: {e}")
            # å¦‚æœ RRF å¤±è´¥ï¼Œè¿”å›åˆå¹¶çš„ç»“æœ
            combined = milvus_results + elasticsearch_results
            return combined


class CrossEncoderReranker:
    """CrossEncoder é‡æ’åºå™¨"""
    
    def __init__(self, model_path: str = "/gemini/pretrain/BAAI/bge-reranker-base"):
        self.logger = get_logger("CrossEncoderReranker")
        self.model_path = model_path
        self.model_name = "BAAI/bge-reranker-base"
        self.model = None
        self._initialize_model()

    def _initialize_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            self.logger.info(f"ğŸ”„ Loading reranker model: {self.model_name}")
            
            # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
            self.device = settings.EMBEDDING_DEVICE
            if self.device == "cuda" and not torch.cuda.is_available():
                self.logger.warning("âš ï¸ CUDA not available, switching to CPU")
                self.device = "cpu"

            # é…ç½® HF é•œåƒä¸ç¦»çº¿æ¨¡å¼
            if settings.HF_MIRROR_BASE_URL:
                os.environ["HF_ENDPOINT"] = settings.HF_MIRROR_BASE_URL
                os.environ["HUGGINGFACE_HUB_BASE_URL"] = settings.HF_MIRROR_BASE_URL
            if settings.EMBEDDING_CACHE_DIR:
                os.environ["TRANSFORMERS_CACHE"] = settings.EMBEDDING_CACHE_DIR
                os.environ["HF_HOME"] = settings.EMBEDDING_CACHE_DIR            
            
            # å°è¯•ä»ç¼“å­˜æˆ–ä¸‹è½½
            if not os.path.exists(settings.EMBEDDING_CACHE_DIR):
                os.mkdir(settings.EMBEDDING_CACHE_DIR)
            self.logger.info(f"Trying to download {self.model_name} to {settings.EMBEDDING_CACHE_DIR} from {settings.HF_MIRROR_BASE_URL}")
            model_path = snapshot_download(
                self.model_name,
                endpoint=settings.HF_MIRROR_BASE_URL,
                cache_dir=settings.EMBEDDING_CACHE_DIR
            )
            self.logger.info(f"model downloaded to {model_path}")

            self.logger.info(f"create reranker model")
            self.logger.info(f"model_path: {model_path}")
            self.logger.info(f"device: {self.device}")
            
            self.model = CrossEncoder(model_path)

            self.logger.info(f"âœ… Reranker model loaded successfully, device: {self.device}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to load reranker model: {e}")
            raise

    
    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
        """
        é‡æ’åºæ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.model or not documents:
            return documents[:top_k]
        
        try:
            # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
            pairs = []
            for doc in documents:
                content = doc.get('content', '')
                if content:
                    pairs.append([query, content])
            
            if not pairs:
                return documents[:top_k]
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            scores = self.model.predict(pairs)
            
            # å°†åˆ†æ•°æ·»åŠ åˆ°æ–‡æ¡£ä¸­
            for i, doc in enumerate(documents):
                doc['rerank_score'] = float(scores[i])
            
            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            reranked_docs = sorted(documents, key=lambda x: x.get('rerank_score', 0), reverse=True)
            
            self.logger.info(f"âœ… Reranked {len(documents)} documents, returning top {top_k}")
            return reranked_docs[:top_k]
            
        except Exception as e:
            self.logger.error(f"âŒ Reranking failed: {e}")
            return documents[:top_k]


class ElasticsearchService:
    """
    Elasticsearch å…³é”®å­—æ£€ç´¢æœåŠ¡
    ä½¿ç”¨ shared.data_access.ElasticsearchClient è¿›è¡Œå…³é”®å­—æ£€ç´¢
    """
    
    def __init__(self):
        self.logger = get_logger("ElasticsearchService")
        self.index_name = getattr(settings, 'ELASTICSEARCH_INDEX', 'k8s-docs')
        self.host = getattr(settings, 'ELASTICSEARCH_HOST', 'http://localhost:9200')
        
        # åˆ›å»ºé…ç½®
        self._config = ElasticsearchConfig(
            es_url=self.host,
            index_name=self.index_name,
            username=getattr(settings, 'ELASTICSEARCH_USER', 'elastic'),
            password=getattr(settings, 'ELASTICSEARCH_PASSWORD', 'password'),
            request_timeout=getattr(settings, 'ELASTICSEARCH_REQUEST_TIMEOUT', 10.0),
            max_retries=getattr(settings, 'ELASTICSEARCH_MAX_RETRIES', 2),
            retry_on_timeout=getattr(settings, 'ELASTICSEARCH_RETRY_ON_TIMEOUT', True),
            enable_highlight=getattr(settings, 'ELASTICSEARCH_ENABLE_HIGHLIGHT', True),
            enable_fuzziness=getattr(settings, 'ELASTICSEARCH_ENABLE_FUZZINESS', True),
        )
        
        # åˆ›å»ºå…±äº«å®¢æˆ·ç«¯
        self._client: Optional[ElasticsearchClient] = None
    
    @property
    def client(self):
        """å‘åå…¼å®¹ï¼šè·å–åº•å±‚å®¢æˆ·ç«¯"""
        return self._client._client if self._client else None
    
    async def initialize(self):
        """åˆå§‹åŒ– Elasticsearch è¿æ¥"""
        try:
            self.logger.info(
                f"ğŸ”„ Connecting to Elasticsearch: {self.host} "
                f"(index={self.index_name})"
            )
            
            self._client = ElasticsearchClient(self._config)
            await self._client.initialize(for_storage=False)  # ç”¨äºæ£€ç´¢
            
            if self._client.is_connected():
                self.logger.info(f"âœ… Connected to Elasticsearch")
            else:
                self.logger.warning("âš ï¸ Elasticsearch not connected, keyword search will be unavailable")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to connect to Elasticsearch: {e}")
            self.logger.error("   The system will continue with vector-only search")
    
    async def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        åœ¨ Elasticsearch ä¸­è¿›è¡Œå…³é”®å­—æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self._client or not self._client.is_connected():
            self.logger.warning("âš ï¸ Elasticsearch client not initialized - falling back to vector-only search")
            return []
        
        try:
            results = await self._client.text_search(query, top_k)
            self.logger.info(f"âœ… Elasticsearch search completed, found {len(results)} results")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Elasticsearch search failed: {e}")
            return []
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        try:
            if self._client:
                await self._client.close()
            self.logger.info("âœ… Elasticsearch connection closed")
        except Exception as e:
            self.logger.error(f"âŒ Failed to close Elasticsearch connection: {e}")


class ContextualCompressionRetriever:
    """ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.logger = get_logger("ContextualCompressionRetriever")
        self.milvus_service = MilvusService()
        self.elasticsearch_service = ElasticsearchService()
        self.embedding_service = EmbeddingService()
        # ä½¿ç”¨é…ç½®ä¸­çš„RRF_Kå€¼
        rrf_k = getattr(settings, 'RRF_K', 60)
        self.rrf_reranker = RRFReranker(k=rrf_k)
        self.cross_encoder_reranker = CrossEncoderReranker()
    
    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡"""
        try:
            await self.milvus_service.initialize()
            await self.elasticsearch_service.initialize()
            self.logger.info("âœ… ContextualCompressionRetriever initialized")
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize ContextualCompressionRetriever: {e}")
            raise
    
    async def retrieve(self, request: RetrievalRequest) -> RetrievalResult:
        """
        æ‰§è¡Œå¤æ‚æ£€ç´¢
        
        Args:
            request: æ£€ç´¢è¯·æ±‚
            
        Returns:
            æ£€ç´¢ç»“æœ
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ” Starting complex retrieval for query: {request.query[:50]}...")
            
            # 1. å¹¶è¡Œæ‰§è¡Œ Milvus å’Œ Elasticsearch æ£€ç´¢
            milvus_task = asyncio.create_task(self._milvus_search(request))
            elasticsearch_task = asyncio.create_task(self._elasticsearch_search(request))
            
            milvus_results, elasticsearch_results = await asyncio.gather(
                milvus_task, elasticsearch_task
            )
            
            # rrf_results = await self._combine_results(
            #     milvus_results,
            #     elasticsearch_results,
            #     request.milvus_weight,
            #     request.elasticsearch_weight
            # )

            # 2. ä½¿ç”¨ RRF ç®—æ³•è¿›è¡Œç¬¬ä¸€æ¬¡é‡æ’åº
            rrf_results = self.rrf_reranker.rerank(milvus_results, elasticsearch_results)
            
            # 3. ä½¿ç”¨ CrossEncoder è¿›è¡Œç¬¬äºŒæ¬¡é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            reranked_results = rrf_results
            if request.use_reranking and len(rrf_results) > 1:
                # å¯¹å‰ top_k*3 ä¸ª RRF ç»“æœè¿›è¡Œ CrossEncoder é‡æ’åºï¼Œæœ€å¤š50ä¸ªï¼Œæé«˜æ£€ç´¢è´¨é‡
                candidates_for_rerank = rrf_results[:min(request.top_k * 3, 50, len(rrf_results))]
                reranked_results = self.cross_encoder_reranker.rerank(
                    request.query, 
                    candidates_for_rerank, 
                    request.rerank_top_k
                )
            
            # 4. è¿”å›æœ€ç»ˆç»“æœ
            final_results = reranked_results[:request.top_k]
            
            execution_time = time.time() - start_time
            
            result = RetrievalResult(
                query=request.query,
                documents=final_results,
                milvus_results=milvus_results,
                elasticsearch_results=elasticsearch_results,
                reranked_results=reranked_results,
                execution_time=execution_time,
                metadata={
                    'milvus_weight': request.milvus_weight,
                    'elasticsearch_weight': request.elasticsearch_weight,
                    'use_reranking': request.use_reranking,
                    'total_milvus_results': len(milvus_results),
                    'total_elasticsearch_results': len(elasticsearch_results),
                    'total_rrf_results': len(rrf_results),
                    'final_results_count': len(final_results),
                    'reranking_method': 'RRF + CrossEncoder' if request.use_reranking else 'RRF only'
                }
            )
            
            self.logger.info(f"âœ… Complex retrieval completed in {execution_time:.2f}s, found {len(final_results)} results")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Complex retrieval failed: {e}")
            raise
    
    async def _milvus_search(self, request: RetrievalRequest) -> List[Dict[str, Any]]:
        """æ‰§è¡Œ Milvus å‘é‡æœç´¢"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_service.encode(request.query)[0]
            
            # åœ¨ Milvus ä¸­æœç´¢
            similar_docs = await self.milvus_service.search_similar(
                query_embedding=query_embedding,
                top_k=request.top_k * 5  # è·å–æ›´å¤šç»“æœç”¨äºåç»­å¤„ç†ï¼Œæé«˜æ£€ç´¢è´¨é‡
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            milvus_results = []
            for doc in similar_docs:
                # è·å–ç»Ÿä¸€çš„ doc_idï¼ˆç”¨äº reranker èåˆï¼‰
                doc_id = doc.get('doc_id') or doc.get('id')
                
                milvus_results.append({
                    'id': doc.get('id'),
                    'doc_id': doc_id,  # ç»Ÿä¸€çš„ doc_id ç”¨äº reranker èåˆ
                    'content': doc.get('content'),
                    'metadata': doc.get('metadata', {}),
                    'score': doc.get('score', 0.0),
                    'source': 'milvus',
                    'file_path': doc.get('file_path', ''),
                    'chunk_index': doc.get('chunk_index', 0),
                    'distance': doc.get('distance', 0.0)
                })
            
            return milvus_results
            
        except Exception as e:
            self.logger.error(f"âŒ Milvus search failed: {e}")
            return []
    
    async def _elasticsearch_search(self, request: RetrievalRequest) -> List[Dict[str, Any]]:
        """æ‰§è¡Œ Elasticsearch æ–‡æœ¬æœç´¢"""
        try:
            # æ£€æŸ¥Elasticsearchè¿æ¥çŠ¶æ€
            if not self.elasticsearch_service.client:
                self.logger.warning("âš ï¸ Elasticsearch not available, skipping keyword search")
                self.logger.warning("   Retrieval will rely on vector search only, which may reduce quality")
                return []
            
            # åœ¨ Elasticsearch ä¸­æœç´¢
            es_results = await self.elasticsearch_service.search(
                query=request.query,
                top_k=request.top_k * 5  # è·å–æ›´å¤šç»“æœç”¨äºåç»­å¤„ç†ï¼Œæé«˜æ£€ç´¢è´¨é‡
            )
            
            if len(es_results) == 0:
                self.logger.warning(f"âš ï¸ Elasticsearch returned no results for query: {request.query[:50]}...")
            
            return es_results
            
        except Exception as e:
            self.logger.error(f"âŒ Elasticsearch search failed: {e}")
            self.logger.error("   Falling back to vector-only search")
            return []
    
    async def _combine_results(
        self, 
        milvus_results: List[Dict[str, Any]], 
        elasticsearch_results: List[Dict[str, Any]],
        milvus_weight: float,
        elasticsearch_weight: float
    ) -> List[Dict[str, Any]]:
        """
        åˆå¹¶å’Œå»é‡ç»“æœ
        
        åŸºäºç»Ÿä¸€çš„ doc_id (æ ¼å¼: relative_path#chunk_index) è¿›è¡Œèåˆ
        """
        try:
            # åˆ›å»º doc_id åˆ°ç»“æœçš„æ˜ å°„
            doc_map = {}
            
            # å¤„ç† Milvus ç»“æœ
            for result in milvus_results:
                # ä½¿ç”¨ doc_id è¿›è¡Œèåˆ
                doc_id = result.get('doc_id') or result.get('id')
                if doc_id:
                    if doc_id not in doc_map:
                        doc_map[doc_id] = {
                            'id': result.get('id', doc_id),
                            'doc_id': doc_id,
                            'content': result.get('content', ''),
                            'metadata': result.get('metadata', {}),
                            'file_path': result.get('file_path', ''),
                            'chunk_index': result.get('chunk_index', 0),
                            'milvus_score': 0.0,
                            'elasticsearch_score': 0.0,
                            'combined_score': 0.0,
                            'sources': []
                        }
                    
                    doc_map[doc_id]['milvus_score'] = result.get('score', 0.0)
                    doc_map[doc_id]['sources'].append('milvus')
            
            # å¤„ç† Elasticsearch ç»“æœ
            for result in elasticsearch_results:
                # ä½¿ç”¨ doc_id è¿›è¡Œèåˆ
                doc_id = result.get('doc_id') or result.get('id')
                if doc_id:
                    if doc_id not in doc_map:
                        doc_map[doc_id] = {
                            'id': result.get('id', doc_id),
                            'doc_id': doc_id,
                            'content': result.get('content', ''),
                            'metadata': {},
                            'file_path': result.get('file_path', ''),
                            'chunk_index': result.get('chunk_index', 0),
                            'milvus_score': 0.0,
                            'elasticsearch_score': 0.0,
                            'combined_score': 0.0,
                            'sources': []
                        }
                    
                    doc_map[doc_id]['elasticsearch_score'] = result.get('score', 0.0)
                    doc_map[doc_id]['sources'].append('elasticsearch')
                    
                    # æ·»åŠ é«˜äº®ä¿¡æ¯
                    if 'highlights' in result:
                        doc_map[doc_id]['highlights'] = result['highlights']
            
            # è®¡ç®—ç»„åˆåˆ†æ•°å¹¶æ’åº
            combined_results = []
            for doc_id, doc in doc_map.items():
                # å½’ä¸€åŒ–åˆ†æ•°
                milvus_score = doc['milvus_score']
                elasticsearch_score = doc['elasticsearch_score']
                
                # è®¡ç®—ç»„åˆåˆ†æ•°
                combined_score = (
                    milvus_score * milvus_weight + 
                    elasticsearch_score * elasticsearch_weight
                )
                
                doc['combined_score'] = combined_score
                combined_results.append(doc)
            
            # æŒ‰ç»„åˆåˆ†æ•°æ’åº
            combined_results.sort(key=lambda x: x['combined_score'], reverse=True)
            
            # ç»Ÿè®¡èåˆæ•ˆæœ
            both_sources = sum(1 for r in combined_results if len(r.get('sources', [])) == 2)
            self.logger.info(
                f"âœ… Combined {len(milvus_results)} Milvus and {len(elasticsearch_results)} "
                f"Elasticsearch results into {len(combined_results)} unique documents "
                f"({both_sources} matched in both sources)"
            )
            return combined_results
            
        except Exception as e:
            self.logger.error(f"âŒ Result combination failed: {e}")
            return []
    
    async def close(self):
        """å…³é—­æ‰€æœ‰æœåŠ¡"""
        try:
            await self.milvus_service.close()
            await self.elasticsearch_service.close()
            self.logger.info("âœ… ContextualCompressionRetriever closed")
        except Exception as e:
            self.logger.error(f"âŒ Failed to close ContextualCompressionRetriever: {e}")


class ComplexRetrievalService:
    """å¤æ‚æ£€ç´¢æœåŠ¡ä¸»ç±»"""
    
    def __init__(self):
        self.logger = get_logger("ComplexRetrievalService")
        self.retriever = ContextualCompressionRetriever()
        self._initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        if not self._initialized:
            await self.retriever.initialize()
            self._initialized = True
            self.logger.info("âœ… ComplexRetrievalService initialized")
    
    async def search(
        self, 
        query: str, 
        top_k: int = 10,
        milvus_weight: float = 0.6,
        elasticsearch_weight: float = 0.4,
        use_reranking: bool = True
    ) -> RetrievalResult:
        """
        æ‰§è¡Œå¤æ‚æ£€ç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            milvus_weight: Milvus ç»“æœæƒé‡
            elasticsearch_weight: Elasticsearch ç»“æœæƒé‡
            use_reranking: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            
        Returns:
            æ£€ç´¢ç»“æœ
        """
        if not self._initialized:
            await self.initialize()
        
        request = RetrievalRequest(
            query=query,
            top_k=top_k,
            milvus_weight=milvus_weight,
            elasticsearch_weight=elasticsearch_weight,
            use_reranking=use_reranking,
            rerank_top_k=top_k * 2
        )
        
        return await self.retriever.retrieve(request)
    
    async def get_document_by_id(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–æ–‡æ¡£"""
        try:
            # é¦–å…ˆå°è¯•ä» Milvus è·å–
            # è¿™é‡Œéœ€è¦å®ç°æ ¹æ®IDæŸ¥è¯¢çš„åŠŸèƒ½
            # æš‚æ—¶è¿”å› None
            return None
        except Exception as e:
            self.logger.error(f"âŒ Failed to get document by ID: {e}")
            return None
    
    async def get_stats(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        try:
            milvus_stats = await self.retriever.milvus_service.get_collection_stats()
            
            return {
                'milvus_stats': milvus_stats,
                'elasticsearch_connected': self.retriever.elasticsearch_service.client is not None,
                'rrf_reranker_available': True,  # RRF é‡æ’åºå™¨æ€»æ˜¯å¯ç”¨çš„
                'cross_encoder_reranker_available': self.retriever.cross_encoder_reranker.model is not None,
                'service_initialized': self._initialized
            }
        except Exception as e:
            self.logger.error(f"âŒ Failed to get stats: {e}")
            return {}
    
    async def close(self):
        """å…³é—­æœåŠ¡"""
        try:
            await self.retriever.close()
            self._initialized = False
            self.logger.info("âœ… ComplexRetrievalService closed")
        except Exception as e:
            self.logger.error(f"âŒ Failed to close ComplexRetrievalService: {e}")

