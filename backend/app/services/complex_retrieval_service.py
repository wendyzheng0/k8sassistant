"""
å¤æ‚æ£€ç´¢æœåŠ¡ - ç»“åˆ Milvus å‘é‡æ•°æ®åº“ã€Elasticsearch å’Œ CrossEncoder é‡æ’åº
"""

import asyncio
import time
import os
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from elasticsearch import Elasticsearch
from elasticsearch.exceptions import ConnectionError as ESConnectionError
from sentence_transformers import CrossEncoder
from huggingface_hub import snapshot_download
import torch

from app.core.config import settings
from app.core.logging import get_logger
from app.services.milvus_service import MilvusService
from app.services.embedding_service import EmbeddingService


@dataclass
class RetrievalRequest:
    """æ£€ç´¢è¯·æ±‚"""
    query: str
    top_k: int = 10
    milvus_weight: float = 0.6
    elasticsearch_weight: float = 0.4
    rerank_top_k: int = 20
    use_reranking: bool = True


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    query: str
    documents: List[Dict[str, Any]]
    milvus_results: List[Dict[str, Any]]
    elasticsearch_results: List[Dict[str, Any]]
    reranked_results: List[Dict[str, Any]]
    execution_time: float
    metadata: Dict[str, Any]


class RRFReranker:
    """RRF (Reciprocal Rank Fusion) é‡æ’åºå™¨"""
    
    def __init__(self, k: int = 60):
        """
        åˆå§‹åŒ– RRF é‡æ’åºå™¨
        
        Args:
            k: RRF ç®—æ³•ä¸­çš„å¸¸æ•°ï¼Œé€šå¸¸è®¾ç½®ä¸º 60
        """
        self.logger = get_logger("RRFReranker")
        self.k = k
    
    def rerank(
        self, 
        milvus_results: List[Dict[str, Any]], 
        elasticsearch_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ RRF ç®—æ³•é‡æ’åºç»“æœ
        
        Args:
            milvus_results: Milvus æœç´¢ç»“æœ
            elasticsearch_results: Elasticsearch æœç´¢ç»“æœ
            
        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ”„ Starting RRF reranking with k={self.k}")
            
            # åˆ›å»ºæ–‡æ¡£IDåˆ°RRFåˆ†æ•°çš„æ˜ å°„
            doc_scores = {}
            
            # å¤„ç† Milvus ç»“æœ
            for rank, doc in enumerate(milvus_results):
                doc_id = doc.get('id')
                if doc_id:
                    if doc_id not in doc_scores:
                        doc_scores[doc_id] = {
                            'id': doc_id,
                            'content': doc.get('content', ''),
                            'metadata': doc.get('metadata', {}),
                            'file_path': doc.get('file_path', ''),
                            'rrf_score': 0.0,
                            'milvus_rank': rank + 1,
                            'elasticsearch_rank': None,
                            'sources': []
                        }
                    
                    # è®¡ç®— RRF åˆ†æ•°ï¼š1 / (k + rank)
                    rrf_score = 1.0 / (self.k + rank + 1)
                    doc_scores[doc_id]['rrf_score'] += rrf_score
                    doc_scores[doc_id]['sources'].append('milvus')
            
            # å¤„ç† Elasticsearch ç»“æœ
            for rank, doc in enumerate(elasticsearch_results):
                doc_id = doc.get('id')
                if doc_id:
                    if doc_id not in doc_scores:
                        doc_scores[doc_id] = {
                            'id': doc_id,
                            'content': doc.get('content', ''),
                            'metadata': {},
                            'file_path': doc.get('file_path', ''),
                            'rrf_score': 0.0,
                            'milvus_rank': None,
                            'elasticsearch_rank': rank + 1,
                            'sources': []
                        }
                    else:
                        doc_scores[doc_id]['elasticsearch_rank'] = rank + 1
                    
                    # è®¡ç®— RRF åˆ†æ•°ï¼š1 / (k + rank)
                    rrf_score = 1.0 / (self.k + rank + 1)
                    doc_scores[doc_id]['rrf_score'] += rrf_score
                    doc_scores[doc_id]['sources'].append('elasticsearch')
                    
                    # æ·»åŠ é«˜äº®ä¿¡æ¯
                    if 'highlights' in doc:
                        doc_scores[doc_id]['highlights'] = doc['highlights']
            
            # æŒ‰ RRF åˆ†æ•°æ’åº
            reranked_results = list(doc_scores.values())
            reranked_results.sort(key=lambda x: x['rrf_score'], reverse=True)
            
            self.logger.info(f"âœ… RRF reranking completed, processed {len(reranked_results)} documents")
            return reranked_results
            
        except Exception as e:
            self.logger.error(f"âŒ RRF reranking failed: {e}")
            # å¦‚æœ RRF å¤±è´¥ï¼Œè¿”å›åˆå¹¶çš„ç»“æœ
            combined = milvus_results + elasticsearch_results
            return combined


class CrossEncoderReranker:
    """CrossEncoder é‡æ’åºå™¨"""
    
    def __init__(self, model_path: str = "/gemini/pretrain/BAAI/bge-reranker-base"):
        self.logger = get_logger("CrossEncoderReranker")
        self.model_path = model_path
        self.model_name = "BAAI/bge-reranker-base"
        self.model = None
        self._initialize_model()

    def _initialize_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            self.logger.info(f"ğŸ”„ Loading reranker model: {self.model_name}")
            
            # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
            self.device = settings.EMBEDDING_DEVICE
            if self.device == "cuda" and not torch.cuda.is_available():
                self.logger.warning("âš ï¸ CUDA not available, switching to CPU")
                self.device = "cpu"

            # é…ç½® HF é•œåƒä¸ç¦»çº¿æ¨¡å¼
            if settings.HF_MIRROR_BASE_URL:
                os.environ["HF_ENDPOINT"] = settings.HF_MIRROR_BASE_URL
                os.environ["HUGGINGFACE_HUB_BASE_URL"] = settings.HF_MIRROR_BASE_URL
            if settings.EMBEDDING_CACHE_DIR:
                os.environ["TRANSFORMERS_CACHE"] = settings.EMBEDDING_CACHE_DIR
                os.environ["HF_HOME"] = settings.EMBEDDING_CACHE_DIR            
            
            # å°è¯•ä»ç¼“å­˜æˆ–ä¸‹è½½
            if not os.path.exists(settings.EMBEDDING_CACHE_DIR):
                os.mkdir(settings.EMBEDDING_CACHE_DIR)
            self.logger.info(f"Trying to download {self.model_name} to {settings.EMBEDDING_CACHE_DIR} from {settings.HF_MIRROR_BASE_URL}")
            model_path = snapshot_download(
                self.model_name,
                endpoint=settings.HF_MIRROR_BASE_URL,
                cache_dir=settings.EMBEDDING_CACHE_DIR
            )
            self.logger.info(f"model downloaded to {model_path}")

            self.logger.info(f"create reranker model")
            self.logger.info(f"model_path: {model_path}")
            self.logger.info(f"device: {self.device}")
            
            self.model = CrossEncoder(model_path)

            self.logger.info(f"âœ… Reranker model loaded successfully, device: {self.device}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to load reranker model: {e}")
            raise

    
    def rerank(self, query: str, documents: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
        """
        é‡æ’åºæ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.model or not documents:
            return documents[:top_k]
        
        try:
            # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
            pairs = []
            for doc in documents:
                content = doc.get('content', '')
                if content:
                    pairs.append([query, content])
            
            if not pairs:
                return documents[:top_k]
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            scores = self.model.predict(pairs)
            
            # å°†åˆ†æ•°æ·»åŠ åˆ°æ–‡æ¡£ä¸­
            for i, doc in enumerate(documents):
                doc['rerank_score'] = float(scores[i])
            
            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            reranked_docs = sorted(documents, key=lambda x: x.get('rerank_score', 0), reverse=True)
            
            self.logger.info(f"âœ… Reranked {len(documents)} documents, returning top {top_k}")
            return reranked_docs[:top_k]
            
        except Exception as e:
            self.logger.error(f"âŒ Reranking failed: {e}")
            return documents[:top_k]


class ElasticsearchService:
    """Elasticsearch æœåŠ¡"""
    
    def __init__(self):
        self.logger = get_logger("ElasticsearchService")
        self.client: Optional[Elasticsearch] = None
        self.index_name = getattr(settings, 'ELASTICSEARCH_INDEX', 'k8s-docs')
        self.host = getattr(settings, 'ELASTICSEARCH_HOST', 'http://localhost:9200')
        self.username = getattr(settings, 'ELASTICSEARCH_USER', 'elastic')
        self.password = getattr(settings, 'ELASTICSEARCH_PASSWORD', 'password')
        self.ca_certs = getattr(settings, 'ELASTICSEARCH_CA_CERTS', None)
    
    async def initialize(self):
        """åˆå§‹åŒ– Elasticsearch è¿æ¥"""
        try:
            self.logger.info(f"ğŸ”„ Connecting to Elasticsearch: {self.host}")
            
            # æ„å»ºè¿æ¥å‚æ•°
            connection_params = {
                'hosts': [self.host],
                'basic_auth': (self.username, self.password)
            }
            
            if self.ca_certs and os.path.exists(self.ca_certs):
                connection_params['ca_certs'] = self.ca_certs
            
            self.client = Elasticsearch(**connection_params)
            
            # æµ‹è¯•è¿æ¥
            info = self.client.info()
            self.logger.info(f"âœ… Connected to Elasticsearch: {info.get('version', {}).get('number', 'unknown')}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to connect to Elasticsearch: {e}")
            self.client = None
    
    async def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        åœ¨ Elasticsearch ä¸­æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.client:
            self.logger.warning("âš ï¸ Elasticsearch client not initialized")
            return []
        
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_body = {
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": ["text^2", "file_path"],
                        "type": "best_fields",
                        "fuzziness": "AUTO"
                    }
                },
                "size": top_k,
                "_source": ["text", "file_path", "chunk_index"],
                "highlight": {
                    "fields": {
                        "text": {
                            "fragment_size": 150,
                            "number_of_fragments": 3
                        }
                    }
                }
            }
            
            # æ‰§è¡Œæœç´¢
            response = self.client.search(
                index=self.index_name,
                body=search_body
            )
            
            # å¤„ç†ç»“æœ
            results = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                highlight = hit.get('highlight', {})
                
                result = {
                    'id': hit['_id'],
                    'content': source.get('text', ''),
                    'file_path': source.get('file_path', ''),
                    'chunk_index': source.get('chunk_index', 0),
                    'score': hit['_score'],
                    'source': 'elasticsearch',
                    'highlights': highlight.get('text', [])
                }
                results.append(result)
            
            self.logger.info(f"âœ… Elasticsearch search completed, found {len(results)} results, requested {top_k}")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Elasticsearch search failed: {e}")
            return []
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        try:
            if self.client:
                self.client.close()
            self.logger.info("âœ… Elasticsearch connection closed")
        except Exception as e:
            self.logger.error(f"âŒ Failed to close Elasticsearch connection: {e}")


class ContextualCompressionRetriever:
    """ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.logger = get_logger("ContextualCompressionRetriever")
        self.milvus_service = MilvusService()
        self.elasticsearch_service = ElasticsearchService()
        self.embedding_service = EmbeddingService()
        self.rrf_reranker = RRFReranker()
        self.cross_encoder_reranker = CrossEncoderReranker()
    
    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡"""
        try:
            await self.milvus_service.initialize()
            await self.elasticsearch_service.initialize()
            self.logger.info("âœ… ContextualCompressionRetriever initialized")
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize ContextualCompressionRetriever: {e}")
            raise
    
    async def retrieve(self, request: RetrievalRequest) -> RetrievalResult:
        """
        æ‰§è¡Œå¤æ‚æ£€ç´¢
        
        Args:
            request: æ£€ç´¢è¯·æ±‚
            
        Returns:
            æ£€ç´¢ç»“æœ
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ” Starting complex retrieval for query: {request.query[:50]}...")
            
            # 1. å¹¶è¡Œæ‰§è¡Œ Milvus å’Œ Elasticsearch æ£€ç´¢
            milvus_task = asyncio.create_task(self._milvus_search(request))
            elasticsearch_task = asyncio.create_task(self._elasticsearch_search(request))
            
            milvus_results, elasticsearch_results = await asyncio.gather(
                milvus_task, elasticsearch_task
            )
            
            # rrf_results = await self._combine_results(
            #     milvus_results,
            #     elasticsearch_results,
            #     request.milvus_weight,
            #     request.elasticsearch_weight
            # )

            # 2. ä½¿ç”¨ RRF ç®—æ³•è¿›è¡Œç¬¬ä¸€æ¬¡é‡æ’åº
            rrf_results = self.rrf_reranker.rerank(milvus_results, elasticsearch_results)
            
            # 3. ä½¿ç”¨ CrossEncoder è¿›è¡Œç¬¬äºŒæ¬¡é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            reranked_results = rrf_results
            if request.use_reranking and len(rrf_results) > 1:
                # åªå¯¹å‰ top_k*2 ä¸ª RRF ç»“æœè¿›è¡Œ CrossEncoder é‡æ’åºï¼Œæé«˜æ•ˆç‡
                candidates_for_rerank = rrf_results[:request.top_k * 2]
                reranked_results = self.cross_encoder_reranker.rerank(
                    request.query, 
                    candidates_for_rerank, 
                    request.rerank_top_k
                )
            
            # 4. è¿”å›æœ€ç»ˆç»“æœ
            final_results = reranked_results[:request.top_k]
            
            execution_time = time.time() - start_time
            
            result = RetrievalResult(
                query=request.query,
                documents=final_results,
                milvus_results=milvus_results,
                elasticsearch_results=elasticsearch_results,
                reranked_results=reranked_results,
                execution_time=execution_time,
                metadata={
                    'milvus_weight': request.milvus_weight,
                    'elasticsearch_weight': request.elasticsearch_weight,
                    'use_reranking': request.use_reranking,
                    'total_milvus_results': len(milvus_results),
                    'total_elasticsearch_results': len(elasticsearch_results),
                    'total_rrf_results': len(rrf_results),
                    'final_results_count': len(final_results),
                    'reranking_method': 'RRF + CrossEncoder' if request.use_reranking else 'RRF only'
                }
            )
            
            self.logger.info(f"âœ… Complex retrieval completed in {execution_time:.2f}s, found {len(final_results)} results")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Complex retrieval failed: {e}")
            raise
    
    async def _milvus_search(self, request: RetrievalRequest) -> List[Dict[str, Any]]:
        """æ‰§è¡Œ Milvus å‘é‡æœç´¢"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_service.encode(request.query)[0]
            
            # åœ¨ Milvus ä¸­æœç´¢
            similar_docs = await self.milvus_service.search_similar(
                query_embedding=query_embedding,
                top_k=request.top_k * 4  # è·å–æ›´å¤šç»“æœç”¨äºåç»­å¤„ç†
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            milvus_results = []
            for doc in similar_docs:
                milvus_results.append({
                    'id': doc.get('id'),
                    'content': doc.get('content'),
                    'metadata': doc.get('metadata', {}),
                    'score': doc.get('score', 0.0),
                    'source': 'milvus',
                    'file_path': doc.get('file_path', ''),
                    'distance': doc.get('distance', 0.0)
                })
            
            return milvus_results
            
        except Exception as e:
            self.logger.error(f"âŒ Milvus search failed: {e}")
            return []
    
    async def _elasticsearch_search(self, request: RetrievalRequest) -> List[Dict[str, Any]]:
        """æ‰§è¡Œ Elasticsearch æ–‡æœ¬æœç´¢"""
        try:
            # åœ¨ Elasticsearch ä¸­æœç´¢
            es_results = await self.elasticsearch_service.search(
                query=request.query,
                top_k=request.top_k * 4  # è·å–æ›´å¤šç»“æœç”¨äºåç»­å¤„ç†
            )
            
            return es_results
            
        except Exception as e:
            self.logger.error(f"âŒ Elasticsearch search failed: {e}")
            return []
    
    async def _combine_results(
        self, 
        milvus_results: List[Dict[str, Any]], 
        elasticsearch_results: List[Dict[str, Any]],
        milvus_weight: float,
        elasticsearch_weight: float
    ) -> List[Dict[str, Any]]:
        """åˆå¹¶å’Œå»é‡ç»“æœ"""
        try:
            # åˆ›å»ºæ–‡æ¡£IDåˆ°ç»“æœçš„æ˜ å°„
            doc_map = {}
            
            # å¤„ç† Milvus ç»“æœ
            for result in milvus_results:
                doc_id = result.get('id')
                if doc_id:
                    if doc_id not in doc_map:
                        doc_map[doc_id] = {
                            'id': doc_id,
                            'content': result.get('content', ''),
                            'metadata': result.get('metadata', {}),
                            'file_path': result.get('file_path', ''),
                            'milvus_score': 0.0,
                            'elasticsearch_score': 0.0,
                            'combined_score': 0.0,
                            'sources': []
                        }
                    
                    doc_map[doc_id]['milvus_score'] = result.get('score', 0.0)
                    doc_map[doc_id]['sources'].append('milvus')
            
            # å¤„ç† Elasticsearch ç»“æœ
            for result in elasticsearch_results:
                doc_id = result.get('id')
                if doc_id:
                    if doc_id not in doc_map:
                        doc_map[doc_id] = {
                            'id': doc_id,
                            'content': result.get('content', ''),
                            'metadata': {},
                            'file_path': result.get('file_path', ''),
                            'milvus_score': 0.0,
                            'elasticsearch_score': 0.0,
                            'combined_score': 0.0,
                            'sources': []
                        }
                    
                    doc_map[doc_id]['elasticsearch_score'] = result.get('score', 0.0)
                    doc_map[doc_id]['sources'].append('elasticsearch')
                    
                    # æ·»åŠ é«˜äº®ä¿¡æ¯
                    if 'highlights' in result:
                        doc_map[doc_id]['highlights'] = result['highlights']
            
            # è®¡ç®—ç»„åˆåˆ†æ•°å¹¶æ’åº
            combined_results = []
            for doc_id, doc in doc_map.items():
                # å½’ä¸€åŒ–åˆ†æ•°
                milvus_score = doc['milvus_score']
                elasticsearch_score = doc['elasticsearch_score']
                
                # è®¡ç®—ç»„åˆåˆ†æ•°
                combined_score = (
                    milvus_score * milvus_weight + 
                    elasticsearch_score * elasticsearch_weight
                )
                
                doc['combined_score'] = combined_score
                combined_results.append(doc)
            
            # æŒ‰ç»„åˆåˆ†æ•°æ’åº
            combined_results.sort(key=lambda x: x['combined_score'], reverse=True)
            
            self.logger.info(f"âœ… Combined {len(milvus_results)} Milvus and {len(elasticsearch_results)} Elasticsearch results into {len(combined_results)} unique documents")
            return combined_results
            
        except Exception as e:
            self.logger.error(f"âŒ Result combination failed: {e}")
            return []
    
    async def close(self):
        """å…³é—­æ‰€æœ‰æœåŠ¡"""
        try:
            await self.milvus_service.close()
            await self.elasticsearch_service.close()
            self.logger.info("âœ… ContextualCompressionRetriever closed")
        except Exception as e:
            self.logger.error(f"âŒ Failed to close ContextualCompressionRetriever: {e}")


class ComplexRetrievalService:
    """å¤æ‚æ£€ç´¢æœåŠ¡ä¸»ç±»"""
    
    def __init__(self):
        self.logger = get_logger("ComplexRetrievalService")
        self.retriever = ContextualCompressionRetriever()
        self._initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        if not self._initialized:
            await self.retriever.initialize()
            self._initialized = True
            self.logger.info("âœ… ComplexRetrievalService initialized")
    
    async def search(
        self, 
        query: str, 
        top_k: int = 10,
        milvus_weight: float = 0.6,
        elasticsearch_weight: float = 0.4,
        use_reranking: bool = True
    ) -> RetrievalResult:
        """
        æ‰§è¡Œå¤æ‚æ£€ç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            milvus_weight: Milvus ç»“æœæƒé‡
            elasticsearch_weight: Elasticsearch ç»“æœæƒé‡
            use_reranking: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            
        Returns:
            æ£€ç´¢ç»“æœ
        """
        if not self._initialized:
            await self.initialize()
        
        request = RetrievalRequest(
            query=query,
            top_k=top_k,
            milvus_weight=milvus_weight,
            elasticsearch_weight=elasticsearch_weight,
            use_reranking=use_reranking,
            rerank_top_k=top_k * 2
        )
        
        return await self.retriever.retrieve(request)
    
    async def get_document_by_id(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–æ–‡æ¡£"""
        try:
            # é¦–å…ˆå°è¯•ä» Milvus è·å–
            # è¿™é‡Œéœ€è¦å®ç°æ ¹æ®IDæŸ¥è¯¢çš„åŠŸèƒ½
            # æš‚æ—¶è¿”å› None
            return None
        except Exception as e:
            self.logger.error(f"âŒ Failed to get document by ID: {e}")
            return None
    
    async def get_stats(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        try:
            milvus_stats = await self.retriever.milvus_service.get_collection_stats()
            
            return {
                'milvus_stats': milvus_stats,
                'elasticsearch_connected': self.retriever.elasticsearch_service.client is not None,
                'rrf_reranker_available': True,  # RRF é‡æ’åºå™¨æ€»æ˜¯å¯ç”¨çš„
                'cross_encoder_reranker_available': self.retriever.cross_encoder_reranker.model is not None,
                'service_initialized': self._initialized
            }
        except Exception as e:
            self.logger.error(f"âŒ Failed to get stats: {e}")
            return {}
    
    async def close(self):
        """å…³é—­æœåŠ¡"""
        try:
            await self.retriever.close()
            self._initialized = False
            self.logger.info("âœ… ComplexRetrievalService closed")
        except Exception as e:
            self.logger.error(f"âŒ Failed to close ComplexRetrievalService: {e}")

