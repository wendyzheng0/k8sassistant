"""
æ–‡æœ¬åµŒå…¥æœåŠ¡
"""

import torch
from typing import List, Union
import os
from app.core.config import settings
from app.core.logging import get_logger
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from huggingface_hub import snapshot_download


class EmbeddingService:
    """æ–‡æœ¬åµŒå…¥æœåŠ¡ç±»"""
    
    def __init__(self):
        self.logger = get_logger("EmbeddingService")
        self.model_name = settings.EMBEDDING_MODEL
        self.device = settings.EMBEDDING_DEVICE
        self.model: HuggingFaceEmbedding = None
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            self.logger.info(f"ğŸ”„ Loading embedding model: {self.model_name}")
            
            # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
            if self.device == "cuda" and not torch.cuda.is_available():
                self.logger.warning("âš ï¸ CUDA not available, switching to CPU")
                self.device = "cpu"

            # é…ç½® HF é•œåƒä¸ç¦»çº¿æ¨¡å¼
            if settings.HF_MIRROR_BASE_URL:
                os.environ["HF_ENDPOINT"] = settings.HF_MIRROR_BASE_URL
                os.environ["HUGGINGFACE_HUB_BASE_URL"] = settings.HF_MIRROR_BASE_URL
            if settings.EMBEDDING_CACHE_DIR:
                os.environ["TRANSFORMERS_CACHE"] = settings.EMBEDDING_CACHE_DIR
                os.environ["HF_HOME"] = settings.EMBEDDING_CACHE_DIR            
            # ç”¨ HuggingFaceEmbedding åŒ…è£…
            model_path = settings.EMBEDDING_MODEL
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æ¨¡å‹ç›®å½•
            local_dir = os.getenv("EMBEDDING_LOCAL_DIR", "").strip()
            if local_dir and os.path.isdir(local_dir):
                model_path = local_dir
                self.logger.info(f"Using local model: {model_path}")
            else:
                # å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œå°è¯•ä»ç¼“å­˜æˆ–ä¸‹è½½
                if not os.path.exists(settings.EMBEDDING_CACHE_DIR):
                    os.mkdir(settings.EMBEDDING_CACHE_DIR)
                self.logger.info(f"Trying to download {settings.EMBEDDING_MODEL} to {settings.EMBEDDING_CACHE_DIR} from {settings.HF_MIRROR_BASE_URL}")
                model_path = snapshot_download(
                    settings.EMBEDDING_MODEL,
                    endpoint=settings.HF_MIRROR_BASE_URL,
                    cache_dir=settings.EMBEDDING_CACHE_DIR
                )
                self.logger.info(f"model downloaded to {model_path}")

            self.logger.info(f"create embedding model")
            self.logger.info(f"model_path: {model_path}")
            self.logger.info(f"device: {settings.EMBEDDING_DEVICE}")
            self.model = HuggingFaceEmbedding(
                model_name=model_path,
                device=self.device
            )

            self.logger.info(f"âœ… Embedding model loaded successfully, device: {self.device}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to load embedding model: {e}")
            raise
    
    def encode(self, texts: Union[str, List[str]]) -> List[List[float]]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        
        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        try:
            if isinstance(texts, str):
                texts = [texts]
            
            # ä½¿ç”¨ HuggingFaceEmbedding çš„ get_text_embedding_batch æ–¹æ³•
            embeddings = self.model.get_text_embedding_batch(texts)
            
            # ç¡®ä¿è¿”å›æ ¼å¼ä¸€è‡´
            if len(texts) == 1:
                return [embeddings[0]]
            else:
                return embeddings
                
        except Exception as e:
            self.logger.error(f"âŒ Text encoding failed: {e}")
            raise
    
    def encode_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        try:
            if not texts:
                return []
            
            all_embeddings = []
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = self.encode(batch_texts)
                all_embeddings.extend(batch_embeddings)
                
                self.logger.debug(f"ğŸ“¦ Processing batch {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}")
            
            self.logger.info(f"âœ… Batch encoding completed, processed {len(texts)} texts")
            return all_embeddings
            
        except Exception as e:
            self.logger.error(f"âŒ Batch encoding failed: {e}")
            raise
    
    def get_embedding_dimension(self) -> int:
        """è·å–åµŒå…¥å‘é‡ç»´åº¦"""
        try:
            # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ–‡æœ¬è·å–ç»´åº¦
            test_embedding = self.model.get_text_embedding("test")
            return len(test_embedding)
        except Exception as e:
            self.logger.error(f"âŒ Failed to get embedding dimension: {e}")
            raise
    
    def similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        try:
            embeddings = self.encode([text1, text2])
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            import numpy as np
            vec1 = np.array(embeddings[0])
            vec2 = np.array(embeddings[1])
            
            similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
            return float(similarity)
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to calculate similarity: {e}")
            raise
    
    def find_most_similar(self, query: str, candidates: List[str], top_k: int = 5) -> List[tuple]:
        """
        æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å€™é€‰æ–‡æœ¬
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidates: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
            
        Returns:
            (æ–‡æœ¬, ç›¸ä¼¼åº¦åˆ†æ•°) çš„åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        try:
            if not candidates:
                return []
            
            # ç¼–ç æ‰€æœ‰æ–‡æœ¬
            all_texts = [query] + candidates
            embeddings = self.encode(all_texts)
            
            query_embedding = embeddings[0]
            candidate_embeddings = embeddings[1:]
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            import numpy as np
            similarities = []
            
            for i, candidate_embedding in enumerate(candidate_embeddings):
                similarity = np.dot(query_embedding, candidate_embedding)
                similarities.append((candidates[i], similarity))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›å‰kä¸ª
            similarities.sort(key=lambda x: x[1], reverse=True)
            return similarities[:top_k]
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to find most similar text: {e}")
            raise
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        try:
            if hasattr(self, 'model') and self.model is not None:
                del self.model
        except:
            pass
