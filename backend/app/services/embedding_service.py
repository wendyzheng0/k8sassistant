"""
æ–‡æœ¬åµŒå…¥æœåŠ¡
"""

import torch
from typing import List, Union
from sentence_transformers import SentenceTransformer
import os
from app.core.config import settings
from app.core.logging import get_logger


class EmbeddingService:
    """æ–‡æœ¬åµŒå…¥æœåŠ¡ç±»"""
    
    def __init__(self):
        self.logger = get_logger("EmbeddingService")
        self.model_name = settings.EMBEDDING_MODEL
        self.device = settings.EMBEDDING_DEVICE
        self.model: SentenceTransformer = None
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            self.logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
            
            # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
            if self.device == "cuda" and not torch.cuda.is_available():
                self.logger.warning("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ° CPU")
                self.device = "cpu"
            
            # é…ç½® HF é•œåƒä¸ç¦»çº¿æ¨¡å¼
            if settings.HF_MIRROR_BASE_URL:
                os.environ["HF_ENDPOINT"] = settings.HF_MIRROR_BASE_URL
                os.environ["HUGGINGFACE_HUB_BASE_URL"] = settings.HF_MIRROR_BASE_URL
            if settings.EMBEDDING_CACHE_DIR:
                os.environ["TRANSFORMERS_CACHE"] = settings.EMBEDDING_CACHE_DIR
                os.environ["HF_HOME"] = settings.EMBEDDING_CACHE_DIR

            # è®¾ç½®ç¦»çº¿æ¨¡å¼
            if settings.HF_OFFLINE:
                os.environ["HF_HUB_OFFLINE"] = "1"
                os.environ["TRANSFORMERS_OFFLINE"] = "1"

            local_dir = settings.EMBEDDING_LOCAL_DIR.strip() if isinstance(settings.EMBEDDING_LOCAL_DIR, str) else ""
            use_local = bool(local_dir) and os.path.isdir(local_dir)
            
            model_source = local_dir if use_local else self.model_name

            # åŠ è½½æ¨¡å‹ï¼ˆä¼˜å…ˆæœ¬åœ°ç›®å½•ï¼Œå…¶æ¬¡ç½‘ç»œ/é•œåƒï¼‰
            if settings.EMBEDDING_CACHE_DIR:
                self.model = SentenceTransformer(
                    model_source,
                    device=self.device,
                    cache_folder=settings.EMBEDDING_CACHE_DIR
                )
            else:
                self.model = SentenceTransformer(
                    model_source,
                    device=self.device
                )
            
            self.logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {self.device}")
            
        except Exception as e:
            self.logger.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def encode(self, texts: Union[str, List[str]]) -> List[List[float]]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        
        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        try:
            if isinstance(texts, str):
                texts = [texts]
            
            # ç¼–ç æ–‡æœ¬
            embeddings = self.model.encode(
                texts,
                convert_to_tensor=False,
                normalize_embeddings=True
            )
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            if len(texts) == 1:
                return [embeddings.tolist()]
            else:
                return embeddings.tolist()
                
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            raise
    
    def encode_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        try:
            if not texts:
                return []
            
            all_embeddings = []
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = self.encode(batch_texts)
                all_embeddings.extend(batch_embeddings)
                
                self.logger.debug(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}")
            
            self.logger.info(f"âœ… æ‰¹é‡ç¼–ç å®Œæˆï¼Œå…±å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬")
            return all_embeddings
            
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡ç¼–ç å¤±è´¥: {e}")
            raise
    
    def get_embedding_dimension(self) -> int:
        """è·å–åµŒå…¥å‘é‡ç»´åº¦"""
        try:
            # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ–‡æœ¬è·å–ç»´åº¦
            test_embedding = self.encode("test")
            return len(test_embedding[0])
        except Exception as e:
            self.logger.error(f"âŒ è·å–åµŒå…¥ç»´åº¦å¤±è´¥: {e}")
            raise
    
    def similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        try:
            embeddings = self.encode([text1, text2])
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            import numpy as np
            vec1 = np.array(embeddings[0])
            vec2 = np.array(embeddings[1])
            
            similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
            return float(similarity)
            
        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            raise
    
    def find_most_similar(self, query: str, candidates: List[str], top_k: int = 5) -> List[tuple]:
        """
        æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å€™é€‰æ–‡æœ¬
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidates: å€™é€‰æ–‡æœ¬åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
            
        Returns:
            (æ–‡æœ¬, ç›¸ä¼¼åº¦åˆ†æ•°) çš„åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        try:
            if not candidates:
                return []
            
            # ç¼–ç æ‰€æœ‰æ–‡æœ¬
            all_texts = [query] + candidates
            embeddings = self.encode(all_texts)
            
            query_embedding = embeddings[0]
            candidate_embeddings = embeddings[1:]
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            import numpy as np
            similarities = []
            
            for i, candidate_embedding in enumerate(candidate_embeddings):
                similarity = np.dot(query_embedding, candidate_embedding)
                similarities.append((candidates[i], similarity))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›å‰kä¸ª
            similarities.sort(key=lambda x: x[1], reverse=True)
            return similarities[:top_k]
            
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥æ‰¾æœ€ç›¸ä¼¼æ–‡æœ¬å¤±è´¥: {e}")
            raise
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        try:
            if hasattr(self, 'model') and self.model is not None:
                del self.model
        except:
            pass
