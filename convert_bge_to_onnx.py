#!/usr/bin/env python3
"""
ä¸“é—¨è½¬æ¢BGEæ¨¡å‹ä¸ºONNXæ ¼å¼
"""

import os
import sys
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
import argparse
from pathlib import Path

def get_expected_dimension(model_name: str) -> int:
    """
    æ ¹æ®æ¨¡å‹åç§°è·å–æœŸæœ›çš„åµŒå…¥ç»´åº¦
    """
    model_name_lower = model_name.lower()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šçš„512ç»´æ¨¡å‹ç‰ˆæœ¬
    if "7999e1d3359715c523056ef9478215996d62a620" in model_name:
        return 512  # è¿™ä¸ªç‰¹å®šçš„BGEæ¨¡å‹ç‰ˆæœ¬ç¡®å®æ˜¯512ç»´
    elif "bge-small" in model_name_lower:
        return 384  # å¤§å¤šæ•°BGE-smallæ¨¡å‹æ˜¯384ç»´
    elif "bge-base" in model_name_lower:
        return 768
    elif "bge-large" in model_name_lower:
        return 1024
    elif "all-minilm-l6-v2" in model_name_lower:
        return 384
    else:
        # é»˜è®¤è¿”å›Noneï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹
        return None

def convert_bge_to_onnx(
    model_name: str = "BAAI/bge-small-zh-v1.5",
    output_dir: str = "bge_onnx_model",
    max_length: int = 512,
    expected_dimension: int = None
):
    """
    è½¬æ¢BGEæ¨¡å‹ä¸ºONNXæ ¼å¼
    """
    
    print(f"ğŸ”„ è½¬æ¢BGEæ¨¡å‹: {model_name}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    if expected_dimension:
        print(f"ğŸ“ æœŸæœ›ç»´åº¦: {expected_dimension}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
        if os.path.exists(model_name):
            print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_name}")
            local_path = model_name
        else:
            print(f"ğŸŒ ä½¿ç”¨è¿œç¨‹æ¨¡å‹: {model_name}")
            local_path = model_name
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        print("ğŸ“¥ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(local_path)
        
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        model = AutoModel.from_pretrained(local_path)
        model.eval()
        model = model.cpu()  # å¼ºåˆ¶CPU
        
        # æ£€æµ‹æ¨¡å‹ç»´åº¦
        print("ğŸ“ æ£€æµ‹æ¨¡å‹ç»´åº¦...")
        model_dimension = model.config.hidden_size
        print(f"âœ… æ¨¡å‹éšè—å±‚ç»´åº¦: {model_dimension}")
        
        # éªŒè¯ç»´åº¦
        if expected_dimension and model_dimension != expected_dimension:
            print(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {expected_dimension}, å®é™… {model_dimension}")
            print("ğŸ’¡ ç»§ç»­è½¬æ¢ï¼Œä½†è¯·æ³¨æ„ç»´åº¦å·®å¼‚")
        elif expected_dimension:
            print(f"âœ… ç»´åº¦åŒ¹é…: {model_dimension}")
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        print("ğŸ”§ åˆ›å»ºç¤ºä¾‹è¾“å…¥...")
        sample_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚"
        
        inputs = tokenizer(
            sample_text,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=max_length
        )
        
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½åœ¨CPUä¸Š
        inputs = {k: v.cpu() for k, v in inputs.items()}
        
        # BGEæ¨¡å‹é€šå¸¸æ”¯æŒtoken_type_ids
        has_token_type_ids = "token_type_ids" in inputs
        print(f"âœ… æ¨¡å‹æ”¯æŒtoken_type_ids: {has_token_type_ids}")
        
        if has_token_type_ids:
            input_names = ["input_ids", "attention_mask", "token_type_ids"]
            onnx_inputs = (
                inputs["input_ids"],
                inputs["attention_mask"], 
                inputs["token_type_ids"]
            )
            dynamic_axes = {
                "input_ids": {0: "batch", 1: "sequence"},
                "attention_mask": {0: "batch", 1: "sequence"},
                "token_type_ids": {0: "batch", 1: "sequence"}
            }
        else:
            input_names = ["input_ids", "attention_mask"]
            onnx_inputs = (
                inputs["input_ids"],
                inputs["attention_mask"]
            )
            dynamic_axes = {
                "input_ids": {0: "batch", 1: "sequence"},
                "attention_mask": {0: "batch", 1: "sequence"}
            }
        
        # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            if has_token_type_ids:
                outputs = model(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    token_type_ids=inputs["token_type_ids"]
                )
            else:
                outputs = model(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"]
                )
        
        print(f"âœ… æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {outputs.last_hidden_state.shape}")
        
        # éªŒè¯è¾“å‡ºç»´åº¦
        actual_dimension = outputs.last_hidden_state.shape[-1]
        print(f"ğŸ“ å®é™…è¾“å‡ºç»´åº¦: {actual_dimension}")
        
        if expected_dimension and actual_dimension != expected_dimension:
            print(f"âŒ è¾“å‡ºç»´åº¦ä¸åŒ¹é…: æœŸæœ› {expected_dimension}, å®é™… {actual_dimension}")
            return False
        elif expected_dimension:
            print(f"âœ… è¾“å‡ºç»´åº¦åŒ¹é…: {actual_dimension}")
        
        # å¯¼å‡ºONNXæ¨¡å‹
        output_path = os.path.join(output_dir, "model.onnx")
        print(f"ğŸ“¤ å¯¼å‡ºONNXæ¨¡å‹åˆ°: {output_path}")
        
        # åˆ é™¤æ—§çš„æ¨¡å‹æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(output_path):
            os.remove(output_path)
            print("ğŸ—‘ï¸ åˆ é™¤æ—§çš„ONNXæ¨¡å‹æ–‡ä»¶")
        
        torch.onnx.export(
            model,
            onnx_inputs,
            output_path,
            input_names=input_names,
            output_names=["last_hidden_state", "pooler_output"],
            dynamic_axes=dynamic_axes,
            opset_version=14,
            do_constant_folding=True,
            export_params=True,
            verbose=False
        )
        
        # éªŒè¯å¯¼å‡ºçš„æ¨¡å‹
        print("ğŸ” éªŒè¯å¯¼å‡ºçš„ONNXæ¨¡å‹...")
        try:
            import onnxruntime as ort
            session = ort.InferenceSession(output_path, providers=['CPUExecutionProvider'])
            print("âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # æ£€æŸ¥è¾“å…¥è¾“å‡º
            print("ğŸ“‹ æ¨¡å‹è¾“å…¥:")
            for input_meta in session.get_inputs():
                print(f"  - {input_meta.name}: {input_meta.shape} ({input_meta.type})")
            
            print("ğŸ“‹ æ¨¡å‹è¾“å‡º:")
            for output_meta in session.get_outputs():
                print(f"  - {output_meta.name}: {output_meta.shape} ({output_meta.type})")
            
            # æµ‹è¯•æ¨ç†
            print("ğŸ§ª æµ‹è¯•ONNXæ¨¡å‹æ¨ç†...")
            if has_token_type_ids:
                onnx_inputs_dict = {
                    "input_ids": inputs["input_ids"].numpy(),
                    "attention_mask": inputs["attention_mask"].numpy(),
                    "token_type_ids": inputs["token_type_ids"].numpy()
                }
            else:
                onnx_inputs_dict = {
                    "input_ids": inputs["input_ids"].numpy(),
                    "attention_mask": inputs["attention_mask"].numpy()
                }
            
            onnx_outputs = session.run(None, onnx_inputs_dict)
            print(f"âœ… ONNXæ¨ç†æˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {onnx_outputs[0].shape}")
            
            # éªŒè¯ONNXæ¨¡å‹è¾“å‡ºç»´åº¦
            onnx_dimension = onnx_outputs[0].shape[-1]
            print(f"ğŸ“ ONNXæ¨¡å‹è¾“å‡ºç»´åº¦: {onnx_dimension}")
            
            if expected_dimension and onnx_dimension != expected_dimension:
                print(f"âŒ ONNXæ¨¡å‹ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {expected_dimension}, å®é™… {onnx_dimension}")
                return False
            elif expected_dimension:
                print(f"âœ… ONNXæ¨¡å‹ç»´åº¦åŒ¹é…: {onnx_dimension}")
            
        except Exception as e:
            print(f"âŒ ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            return False
        
        # ä¿å­˜tokenizeré…ç½®
        print("ğŸ’¾ ä¿å­˜tokenizeré…ç½®...")
        tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜æ¨¡å‹é…ç½®
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹é…ç½®...")
        model.config.save_pretrained(output_dir)
        
        # ä¿å­˜ç»´åº¦ä¿¡æ¯
        print("ğŸ’¾ ä¿å­˜ç»´åº¦ä¿¡æ¯...")
        dimension_info = {
            "model_name": model_name,
            "hidden_size": model_dimension,
            "output_dimension": actual_dimension,
            "expected_dimension": expected_dimension,
            "max_length": max_length,
            "conversion_timestamp": str(np.datetime64('now'))
        }
        
        import json
        dimension_file = os.path.join(output_dir, "dimension_info.json")
        with open(dimension_file, 'w', encoding='utf-8') as f:
            json.dump(dimension_info, f, indent=2, ensure_ascii=False)
        
        print("ğŸ‰ BGEæ¨¡å‹è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“„ ONNXæ¨¡å‹: {output_path}")
        print(f"ğŸ“„ ç»´åº¦ä¿¡æ¯: {dimension_file}")
        print(f"ğŸ“ æœ€ç»ˆç»´åº¦: {actual_dimension}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    parser = argparse.ArgumentParser(description='è½¬æ¢BGEæ¨¡å‹ä¸ºONNXæ ¼å¼')
    parser.add_argument('--model-name', type=str, default='BAAI/bge-small-zh-v1.5',
                        help='BGEæ¨¡å‹åç§°')
    parser.add_argument('--output-dir', type=str, default='bge_onnx_model',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-length', type=int, default=512,
                        help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--expected-dimension', type=int, default=None,
                        help='æœŸæœ›çš„åµŒå…¥ç»´åº¦ (ä¾‹å¦‚: 384 for bge-small, 768 for bge-base)')
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæœŸæœ›ç»´åº¦ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹
    expected_dimension = args.expected_dimension
    if expected_dimension is None:
        expected_dimension = get_expected_dimension(args.model_name)
        if expected_dimension:
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æœŸæœ›ç»´åº¦: {expected_dimension}")
    
    success = convert_bge_to_onnx(
        model_name=args.model_name,
        output_dir=args.output_dir,
        max_length=args.max_length,
        expected_dimension=expected_dimension
    )
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())
