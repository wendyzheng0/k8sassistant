"""
Elasticsearch Data Access Client
ç»Ÿä¸€çš„ Elasticsearch æ•°æ®è®¿é—®å±‚ï¼Œæ”¯æŒå­˜å‚¨å’Œå…³é”®å­—æ£€ç´¢

åŒæ—¶ä¾› data_processing (å­˜å‚¨) å’Œ backend (å…³é”®å­—æ£€ç´¢) ä½¿ç”¨
"""

import asyncio
import os
import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from elasticsearch import Elasticsearch
from llama_index.core.schema import TextNode
from llama_index.vector_stores.elasticsearch import ElasticsearchStore


@dataclass
class ElasticsearchConfig:
    """Elasticsearch é…ç½®"""
    es_url: str = ""
    index_name: str = ""
    username: str = ""
    password: str = ""
    batch_size: int = 200
    
    # æœç´¢é…ç½®
    request_timeout: float = 10.0
    max_retries: int = 2
    retry_on_timeout: bool = True
    enable_highlight: bool = True
    enable_fuzziness: bool = True
    
    def __post_init__(self):
        """ä» shared config åŠ è½½é»˜è®¤å€¼"""
        from shared.config import get_settings
        settings = get_settings()
        
        if not self.es_url:
            self.es_url = settings.ELASTICSEARCH_HOST
        if not self.index_name:
            self.index_name = settings.ELASTICSEARCH_INDEX
        if not self.username:
            self.username = settings.ELASTICSEARCH_USER
        if not self.password:
            self.password = settings.ELASTICSEARCH_PASSWORD


@dataclass
class StorageResult:
    """å­˜å‚¨æ“ä½œç»“æœ"""
    success: bool = True
    stored_count: int = 0
    error_count: int = 0
    errors: List[str] = field(default_factory=list)
    
    def add_error(self, error: str):
        self.errors.append(error)
        self.error_count += 1
        self.success = False


class ElasticsearchClient:
    """
    Elasticsearch ç»Ÿä¸€å®¢æˆ·ç«¯
    
    æä¾›ä¸¤ç§ä½¿ç”¨æ–¹å¼:
    1. ä½¿ç”¨ LlamaIndex ElasticsearchStore è¿›è¡Œæ–‡æ¡£å­˜å‚¨ï¼ˆé€‚åˆ data_processingï¼‰
    2. ä½¿ç”¨ elasticsearch-py è¿›è¡Œå…³é”®å­—æ£€ç´¢ï¼ˆé€‚åˆ backendï¼‰
    """
    
    def __init__(self, config: Optional[ElasticsearchConfig] = None):
        self.config = config or ElasticsearchConfig()
        self.logger = logging.getLogger("shared.elasticsearch")
        
        # elasticsearch-py å®¢æˆ·ç«¯ï¼ˆç”¨äºæ£€ç´¢ï¼‰
        self._client: Optional[Elasticsearch] = None
        
        # LlamaIndex ElasticsearchStoreï¼ˆç”¨äºå­˜å‚¨ï¼‰
        self._vector_store: Optional[ElasticsearchStore] = None
        
        self._initialized = False
    
    async def initialize(self, for_storage: bool = False) -> None:
        """
        åˆå§‹åŒ– Elasticsearch è¿æ¥
        
        Args:
            for_storage: æ˜¯å¦ç”¨äºå­˜å‚¨ï¼ˆä¼šåˆå§‹åŒ– LlamaIndex Storeï¼‰
        """
        if self._initialized:
            return
        
        try:
            self.logger.info(f"ğŸ”— Connecting to Elasticsearch: {self.config.es_url}")
            
            # åˆå§‹åŒ– elasticsearch-py å®¢æˆ·ç«¯
            connection_params = {
                'hosts': [self.config.es_url],
                'basic_auth': (self.config.username, self.config.password),
                'request_timeout': self.config.request_timeout,
                'max_retries': self.config.max_retries,
                'retry_on_timeout': self.config.retry_on_timeout,
            }
            
            self._client = Elasticsearch(**connection_params)
            
            # æµ‹è¯•è¿æ¥
            info = await asyncio.to_thread(self._client.info)
            self.logger.info(f"âœ… Connected to Elasticsearch: {info.get('version', {}).get('number', 'unknown')}")
            
            if for_storage:
                # åˆå§‹åŒ– LlamaIndex Store ç”¨äºå­˜å‚¨
                self._vector_store = ElasticsearchStore(
                    es_url=self.config.es_url,
                    index_name=self.config.index_name,
                    es_user=self.config.username,
                    es_password=self.config.password,
                )
            
            self._initialized = True
            self.logger.info(f"âœ… Elasticsearch initialized: {self.config.index_name}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize Elasticsearch: {e}")
            self.logger.info("ğŸ’¡ Troubleshooting tips:")
            self.logger.info("   1. Check if Elasticsearch is running")
            self.logger.info(f"   2. Verify the host URL ({self.config.es_url})")
            self.logger.info("   3. Check username/password credentials")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç³»ç»Ÿåœ¨æ²¡æœ‰ ES çš„æƒ…å†µä¸‹ç»§ç»­è¿è¡Œ
            self._client = None
    
    # ==================== å­˜å‚¨æ“ä½œ ====================
    
    async def store_documents(self, documents: List[Dict[str, Any]]) -> StorageResult:
        """
        å­˜å‚¨æ–‡æ¡£åˆ° Elasticsearch
        
        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰:
        1. æœ‰ embedding æ—¶ï¼šä½¿ç”¨ LlamaIndex ElasticsearchStoreï¼ˆå‘é‡+å…³é”®å­—æ··åˆæ£€ç´¢ï¼‰
        2. æ—  embedding æ—¶ï¼šä½¿ç”¨ elasticsearch-py ç›´æ¥å­˜å‚¨ï¼ˆçº¯å…³é”®å­—/BM25 æ£€ç´¢ï¼‰
        
        æ³¨æ„ï¼šLlamaIndex çš„ ElasticsearchStore.add() å³ä½¿è®¾ç½® BM25Strategyï¼Œ
        å­˜å‚¨æ—¶ä»ç„¶è¦æ±‚ embeddingï¼Œå› æ­¤çº¯å…³é”®å­—æ¨¡å¼å¿…é¡»ç”¨ elasticsearch-pyã€‚
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« id, content, metadataï¼ˆembedding å¯é€‰ï¼‰
            
        Returns:
            StorageResult: å­˜å‚¨ç»“æœ
        """
        result = StorageResult()
        
        if not documents:
            self.logger.warning("âš ï¸ No documents to store")
            return result
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ embeddingï¼Œå†³å®šä½¿ç”¨å“ªç§å­˜å‚¨æ–¹å¼
        has_embeddings = any(doc.get("embedding") for doc in documents)
        
        if has_embeddings:
            # æœ‰ embeddingï¼šä½¿ç”¨ LlamaIndex ElasticsearchStore
            return await self._store_with_llamaindex(documents, result)
        else:
            # æ—  embeddingï¼šç›´æ¥ç”¨ elasticsearch-py å­˜å‚¨ï¼ˆBM25 å…³é”®å­—æ£€ç´¢ï¼‰
            return await self._store_text_only(documents, result)
    
    async def _store_with_llamaindex(self, documents: List[Dict[str, Any]], result: StorageResult) -> StorageResult:
        """ä½¿ç”¨ LlamaIndex ElasticsearchStore å­˜å‚¨å¸¦å‘é‡çš„æ–‡æ¡£"""
        if not self._vector_store:
            self._vector_store = ElasticsearchStore(
                es_url=self.config.es_url,
                index_name=self.config.index_name,
                es_user=self.config.username,
                es_password=self.config.password,
            )
        
        nodes: List[TextNode] = []
        for doc in documents:
            doc_id = doc.get("id")
            content = doc.get("content")
            embedding = doc.get("embedding")
            
            if not doc_id or not content or not embedding:
                result.add_error("Invalid document: missing id, content, or embedding")
                continue
            
            nodes.append(TextNode(
                id_=doc_id,
                text=content,
                embedding=embedding,
                metadata=doc.get("metadata", {}),
            ))
        
        if not nodes:
            result.add_error("No valid documents to store")
            return result
        
        try:
            total_stored = 0
            for i in range(0, len(nodes), self.config.batch_size):
                batch = nodes[i:i + self.config.batch_size]
                self._vector_store.add(batch)
                total_stored += len(batch)
                self.logger.debug(f"ğŸ“¦ Stored batch {i // self.config.batch_size + 1}")
            
            result.stored_count = total_stored
            self.logger.info(f"âœ… Stored {total_stored} documents with embeddings to Elasticsearch")
        except Exception as e:
            result.add_error(f"Failed to store: {str(e)}")
        
        return result
    
    async def _store_text_only(self, documents: List[Dict[str, Any]], result: StorageResult) -> StorageResult:
        """
        ä½¿ç”¨ elasticsearch-py ç›´æ¥å­˜å‚¨çº¯æ–‡æœ¬æ–‡æ¡£ï¼ˆBM25 å…³é”®å­—æ£€ç´¢ï¼‰
        
        LlamaIndex çš„ ElasticsearchStore.add() å³ä½¿ç”¨ BM25Strategyï¼Œ
        ä»ç„¶ä¼šæ£€æŸ¥ embeddingï¼Œå› æ­¤çº¯æ–‡æœ¬å¿…é¡»ç”¨åŸç”Ÿ ES å®¢æˆ·ç«¯ã€‚
        
        æ³¨æ„ï¼šä½¿ç”¨ä¸ Milvus ç›¸åŒçš„ doc_id æ ¼å¼ï¼ˆrelative_path#chunk_indexï¼‰ï¼Œ
        ä»¥ä¾¿åœ¨ RRF reranker ä¸­æ­£ç¡®èåˆä¸¤è¾¹çš„ç»“æœã€‚
        """
        if not self._client:
            result.add_error("Elasticsearch client not initialized")
            return result
        
        # ç¡®ä¿ç´¢å¼•å­˜åœ¨ï¼Œä½¿ç”¨é€‚åˆå…³é”®å­—æ£€ç´¢çš„ mapping
        await self._ensure_text_index_exists()
        
        try:
            total_stored = 0
            for i in range(0, len(documents), self.config.batch_size):
                batch = documents[i:i + self.config.batch_size]
                actions = []
                
                for doc in batch:
                    doc_id = doc.get("id")
                    content = doc.get("content", "")
                    metadata = doc.get("metadata", {})
                    
                    if not doc_id or not content:
                        result.add_error("Invalid document: missing id or content")
                        continue
                    
                    # è·å–æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œç¡®ä¿ä¸€è‡´æ€§ï¼‰
                    file_path = metadata.get("relative_path", "") or metadata.get("file_path", "")
                    chunk_index = metadata.get("chunk_index", 0)
                    
                    # ä½¿ç”¨ç»Ÿä¸€çš„ doc_id æ ¼å¼ï¼ˆä¸ Milvus ä¿æŒä¸€è‡´ï¼‰
                    # ä¼˜å…ˆä½¿ç”¨ metadata ä¸­çš„ doc_idï¼Œå¦åˆ™ä½¿ç”¨ä¼ å…¥çš„ id
                    unified_doc_id = metadata.get("doc_id", doc_id)
                    
                    # æ„å»º ES æ–‡æ¡£
                    es_doc = {
                        "text": content,
                        "file_path": file_path,  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
                        "chunk_index": chunk_index,
                        "source": metadata.get("source", ""),
                        "doc_id": unified_doc_id,  # ç»Ÿä¸€çš„ doc_id ç”¨äº reranker èåˆ
                    }
                    
                    # ä½¿ç”¨ unified_doc_id ä½œä¸º ES æ–‡æ¡£ ID
                    actions.append({"index": {"_index": self.config.index_name, "_id": unified_doc_id}})
                    actions.append(es_doc)
                
                if actions:
                    await asyncio.to_thread(
                        self._client.bulk,
                        body=actions,
                        refresh=True
                    )
                    total_stored += len(batch)
                    self.logger.debug(f"ğŸ“¦ Stored batch {i // self.config.batch_size + 1}")
            
            result.stored_count = total_stored
            self.logger.info(f"âœ… Stored {total_stored} text-only documents to Elasticsearch (BM25)")
        except Exception as e:
            result.add_error(f"Failed to store: {str(e)}")
        
        return result
    
    async def _ensure_text_index_exists(self) -> None:
        """ç¡®ä¿çº¯æ–‡æœ¬ç´¢å¼•å­˜åœ¨ï¼Œåˆ›å»ºé€‚åˆ BM25 å…³é”®å­—æ£€ç´¢çš„ mapping"""
        try:
            index_exists = await asyncio.to_thread(
                self._client.indices.exists,
                index=self.config.index_name
            )
            if not index_exists:
                mapping = {
                    "mappings": {
                        "properties": {
                            "text": {"type": "text", "analyzer": "standard"},
                            "file_path": {"type": "keyword"},
                            "chunk_index": {"type": "integer"},
                            "source": {"type": "keyword"},
                            "doc_id": {"type": "keyword"},
                        }
                    }
                }
                await asyncio.to_thread(
                    self._client.indices.create,
                    index=self.config.index_name,
                    body=mapping
                )
                self.logger.info(f"âœ… Created text index: {self.config.index_name}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Index check/create warning: {e}")
    
    # ==================== å…³é”®å­—æ£€ç´¢æ“ä½œ ====================
    
    async def text_search(
        self,
        query: str,
        top_k: int = 10,
    ) -> List[Dict[str, Any]]:
        """
        å…³é”®å­—/æ–‡æœ¬æ£€ç´¢ï¼ˆBM25ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self._client:
            self.logger.warning("âš ï¸ Elasticsearch client not initialized")
            return []
        
        try:
            # æ ¹æ® query é•¿åº¦å†³å®šæ˜¯å¦å¯ç”¨æ¨¡ç³ŠåŒ¹é…
            token_count = len(query.split())
            use_fuzziness = self.config.enable_fuzziness and token_count <= 8
            use_highlight = self.config.enable_highlight and top_k <= 20
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            multi_match: Dict[str, Any] = {
                "query": query,
                "fields": ["text^2", "file_path"],
                "type": "best_fields",
            }
            
            if use_fuzziness:
                multi_match.update({
                    "fuzziness": "AUTO",
                    "max_expansions": 50,
                    "prefix_length": 1,
                })
            
            search_body = {
                "query": {
                    "multi_match": multi_match
                },
                "size": top_k,
                "_source": ["text", "file_path", "chunk_index", "doc_id"],  # æ·»åŠ  doc_id ç”¨äº reranker èåˆ
                "track_total_hits": False,
            }
            
            if use_highlight:
                search_body["highlight"] = {
                    "fields": {
                        "text": {
                            "fragment_size": 150,
                            "number_of_fragments": 3,
                        }
                    }
                }
            
            # æ‰§è¡Œæœç´¢
            response = await asyncio.to_thread(
                self._client.search,
                index=self.config.index_name,
                body=search_body,
                request_timeout=self.config.request_timeout,
            )
            
            # å¤„ç†ç»“æœ
            results = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                highlight = hit.get('highlight', {})
                
                # è·å–ç»Ÿä¸€çš„ doc_idï¼ˆç”¨äº reranker èåˆï¼‰
                # ä¼˜å…ˆä½¿ç”¨å­˜å‚¨æ—¶ä¿å­˜çš„ doc_idï¼Œå¦åˆ™ä½¿ç”¨ ES æ–‡æ¡£ ID
                doc_id = source.get('doc_id') or hit['_id']
                
                results.append({
                    'id': hit['_id'],
                    'doc_id': doc_id,  # æ·»åŠ  doc_id ç”¨äº reranker èåˆ
                    'content': source.get('text', ''),
                    'file_path': source.get('file_path', ''),
                    'chunk_index': source.get('chunk_index', 0),
                    'score': hit['_score'],
                    'source': 'elasticsearch',
                    'highlights': highlight.get('text', [])
                })
            
            self.logger.info(f"ğŸ” Text search completed, found {len(results)} results")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Text search failed: {e}")
            return []
    
    async def get_index_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        if not self._client:
            return {"status": "not_initialized"}
        
        try:
            index_stats = await asyncio.to_thread(
                self._client.indices.stats,
                index=self.config.index_name
            )
            doc_count = index_stats["_all"]["primaries"]["docs"]["count"]
            
            return {
                "index_name": self.config.index_name,
                "doc_count": doc_count,
                "status": "exists",
            }
        except Exception as e:
            self.logger.warning(f"âš ï¸ Failed to get index stats: {e}")
            return {"status": "error", "error": str(e)}
    
    def is_connected(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è¿æ¥"""
        return self._client is not None
    
    async def close(self) -> None:
        """å…³é—­è¿æ¥"""
        try:
            if self._client:
                self._client.close()
            self._initialized = False
            self.logger.info("âœ… Elasticsearch connection closed")
        except Exception as e:
            self.logger.error(f"âŒ Failed to close: {e}")

