"""
Milvus Data Access Client
ç»Ÿä¸€çš„ Milvus æ•°æ®è®¿é—®å±‚ï¼Œæ”¯æŒå­˜å‚¨å’Œå‘é‡æ£€ç´¢

åŒæ—¶ä¾› data_processing (å­˜å‚¨) å’Œ backend (æ£€ç´¢) ä½¿ç”¨
"""

import asyncio
import os
import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

from pymilvus import MilvusClient as PyMilvusClient, connections, Collection, CollectionSchema, FieldSchema, DataType
from llama_index.core.schema import TextNode
from llama_index.vector_stores.milvus import MilvusVectorStore


@dataclass
class MilvusConfig:
    """Milvus é…ç½®"""
    uri: str = ""
    collection_name: str = ""
    vector_dim: int = 0
    similarity_metric: str = "COSINE"
    overwrite: bool = False  # æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„é›†åˆï¼ˆä»…ç”¨äºå­˜å‚¨ï¼‰
    
    def __post_init__(self):
        """ä» shared config åŠ è½½é»˜è®¤å€¼"""
        if not self.uri or not self.collection_name or not self.vector_dim:
            from shared.config import get_settings
            settings = get_settings()
            
            if not self.uri:
                self.uri = settings.MILVUS_URI
            if not self.collection_name:
                self.collection_name = settings.COLLECTION_NAME
            if not self.vector_dim:
                self.vector_dim = settings.VECTOR_DIM


@dataclass
class StorageResult:
    """å­˜å‚¨æ“ä½œç»“æœ"""
    success: bool = True
    stored_count: int = 0
    error_count: int = 0
    errors: List[str] = field(default_factory=list)
    
    def add_error(self, error: str):
        self.errors.append(error)
        self.error_count += 1
        self.success = False


class MilvusClient:
    """
    Milvus ç»Ÿä¸€å®¢æˆ·ç«¯
    
    æä¾›ä¸¤ç§ä½¿ç”¨æ–¹å¼:
    1. ä½¿ç”¨ LlamaIndex MilvusVectorStore è¿›è¡Œæ‰¹é‡å­˜å‚¨ï¼ˆé€‚åˆ data_processingï¼‰
    2. ä½¿ç”¨ pymilvus è¿›è¡Œçµæ´»çš„å‘é‡æ£€ç´¢ï¼ˆé€‚åˆ backendï¼‰
    """
    
    def __init__(self, config: Optional[MilvusConfig] = None):
        self.config = config or MilvusConfig()
        self.logger = logging.getLogger("shared.milvus")
        
        # pymilvus å®¢æˆ·ç«¯ï¼ˆç”¨äºæ£€ç´¢ï¼‰
        self._client: Optional[PyMilvusClient] = None
        
        # LlamaIndex VectorStoreï¼ˆç”¨äºå­˜å‚¨ï¼‰
        self._vector_store: Optional[MilvusVectorStore] = None
        
        self._initialized = False
    
    async def initialize(self, for_storage: bool = False) -> None:
        """
        åˆå§‹åŒ– Milvus è¿æ¥
        
        Args:
            for_storage: æ˜¯å¦ç”¨äºå­˜å‚¨ï¼ˆä¼šåˆå§‹åŒ– LlamaIndex VectorStoreï¼‰
        """
        if self._initialized:
            return
        
        try:
            self.logger.info(f"ğŸ”— Connecting to Milvus: {self.config.uri}")
            
            # è§£æ URI
            raw_uri = self.config.uri.strip()
            if "://" in raw_uri:
                parsed = urlparse(raw_uri)
                host = parsed.hostname
                port = parsed.port
                client_uri = f"{parsed.scheme}://{host}:{port}"
            else:
                if ":" in raw_uri:
                    host, port_str = raw_uri.rsplit(":", 1)
                    port = int(port_str)
                else:
                    host = raw_uri
                    port = 19530
                client_uri = f"http://{host}:{port}"
            
            # è¿æ¥ pymilvus
            connections.connect(alias="default", host=host, port=port)
            self._client = PyMilvusClient(uri=client_uri, token="")
            
            if for_storage:
                # åˆå§‹åŒ– LlamaIndex VectorStore ç”¨äºå­˜å‚¨
                self._vector_store = MilvusVectorStore(
                    uri=self.config.uri,
                    collection_name=self.config.collection_name,
                    dim=self.config.vector_dim,
                    overwrite=self.config.overwrite,
                    similarity_metric=self.config.similarity_metric,
                )
            else:
                # ç¡®ä¿é›†åˆå­˜åœ¨ï¼ˆç”¨äºæ£€ç´¢åœºæ™¯ï¼‰
                await self._ensure_collection_exists()
            
            self._initialized = True
            self.logger.info(f"âœ… Milvus initialized: {self.config.collection_name}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize Milvus: {e}")
            raise
    
    async def _ensure_collection_exists(self) -> None:
        """ç¡®ä¿é›†åˆå­˜åœ¨"""
        try:
            collections = self._client.list_collections()
            if self.config.collection_name not in collections:
                self.logger.warning(f"âš ï¸ Collection {self.config.collection_name} does not exist")
                # åˆ›å»ºé›†åˆ
                fields = [
                    FieldSchema(name="id", dtype=DataType.VARCHAR, max_length=65535, is_primary=True),
                    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
                    FieldSchema(name="metadata", dtype=DataType.JSON),
                    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.config.vector_dim)
                ]
                schema = CollectionSchema(fields=fields, description="Vector store")
                self._client.create_collection(
                    collection_name=self.config.collection_name,
                    schema=schema
                )
                
                # åˆ›å»ºç´¢å¼•
                collection = Collection(self.config.collection_name)
                collection.create_index(
                    field_name="embedding",
                    index_params={
                        "index_type": "IVF_FLAT",
                        "metric_type": self.config.similarity_metric,
                        "params": {"nlist": 1024}
                    }
                )
                collection.load()
                self.logger.info(f"âœ… Created collection: {self.config.collection_name}")
            else:
                # åŠ è½½å·²å­˜åœ¨çš„é›†åˆ
                collection = Collection(self.config.collection_name)
                collection.load()
                self.logger.info(f"âœ… Collection exists: {self.config.collection_name}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error ensuring collection exists: {e}")
    
    # ==================== å­˜å‚¨æ“ä½œ ====================
    
    async def store_documents(self, documents: List[Dict[str, Any]]) -> StorageResult:
        """
        å­˜å‚¨æ–‡æ¡£åˆ° Milvusï¼ˆä½¿ç”¨ LlamaIndexï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« id, content, embedding, metadata
                      metadata ä¸­åº”åŒ…å« doc_id (æ ¼å¼: relative_path#chunk_index) ç”¨äº reranker èåˆ
            
        Returns:
            StorageResult: å­˜å‚¨ç»“æœ
        """
        result = StorageResult()
        
        if not documents:
            self.logger.warning("âš ï¸ No documents to store")
            return result
        
        if not self._vector_store:
            # å¦‚æœæ²¡æœ‰åˆå§‹åŒ– vector_storeï¼Œåˆå§‹åŒ–å®ƒ
            self._vector_store = MilvusVectorStore(
                uri=self.config.uri,
                collection_name=self.config.collection_name,
                dim=self.config.vector_dim,
                overwrite=self.config.overwrite,
                similarity_metric=self.config.similarity_metric,
            )
        
        # è½¬æ¢ä¸º LlamaIndex TextNode
        nodes: List[TextNode] = []
        for doc in documents:
            if not doc.get("id") or not doc.get("content") or not doc.get("embedding"):
                result.add_error("Invalid document: missing required fields")
                continue
            
            content = doc["content"]
            if len(content) > 65000:
                content = content[:65000]
            
            metadata = doc.get("metadata", {}).copy()
            
            # ç¡®ä¿ doc_id å­˜åœ¨äº metadata ä¸­ï¼ˆç”¨äº reranker èåˆï¼‰
            # ä¼˜å…ˆä½¿ç”¨ metadata ä¸­çš„ doc_idï¼Œå¦åˆ™ä½¿ç”¨æ–‡æ¡£ id
            if "doc_id" not in metadata:
                metadata["doc_id"] = doc["id"]
            
            node = TextNode(
                id_=doc["id"],  # ä½¿ç”¨ç»Ÿä¸€çš„ doc_id ä½œä¸ºä¸»é”®
                text=content,
                embedding=doc["embedding"],
                metadata=metadata,
            )
            nodes.append(node)
        
        if not nodes:
            result.add_error("No valid documents to store")
            return result
        
        try:
            self._vector_store.add(nodes)
            result.stored_count = len(nodes)
            self.logger.info(f"âœ… Stored {len(nodes)} documents to Milvus")
        except Exception as e:
            result.add_error(f"Failed to store: {str(e)}")
        
        return result
    
    # ==================== æ£€ç´¢æ“ä½œ ====================
    
    async def search_similar(
        self,
        query_embedding: List[float],
        top_k: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self._client:
            self.logger.error("Milvus client not initialized")
            return []
        
        try:
            # åŠ¨æ€è®¡ç®— nprobe
            try:
                collection_info = self._client.describe_collection(self.config.collection_name)
                row_count = collection_info.get("num_rows", 0) or collection_info.get("row_count", 0)
                
                if row_count < 1000:
                    nprobe = 10
                elif row_count < 10000:
                    nprobe = 32
                elif row_count < 100000:
                    nprobe = 64
                else:
                    nprobe = 128
            except Exception:
                nprobe = 32
            
            # æ‰§è¡Œæœç´¢
            results = self._client.search(
                collection_name=self.config.collection_name,
                data=[query_embedding],
                search_params={
                    "metric_type": self.config.similarity_metric,
                    "params": {"nprobe": nprobe}
                },
                limit=top_k,
                output_fields=["*"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            search_results = []
            if results and len(results) > 0:
                for result in results[0]:
                    entity = result.get("entity", {})
                    
                    # å¤„ç†åˆ†æ•°
                    score = result.get("score") or result.get("distance")
                    if result.get("distance") is not None and result.get("score") is None:
                        distance = result.get("distance")
                        score = 1.0 / (1.0 + distance) if distance > 0 else 1.0
                    
                    # æå–å†…å®¹
                    content = entity.get("content", entity.get("text", ""))
                    
                    # è·å– metadataï¼ˆLlamaIndex å­˜å‚¨æ—¶å¯èƒ½æ”¾åœ¨ entity.metadata æˆ–ç›´æ¥åœ¨ entity ä¸­ï¼‰
                    metadata = entity.get("metadata", {})
                    
                    # æå– file_pathï¼ˆä¼˜å…ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œä¸ doc_id ä¿æŒä¸€è‡´ï¼‰
                    file_path = (
                        metadata.get("relative_path", "") or 
                        metadata.get("file_path", "") or 
                        entity.get("file_path", "unknown")
                    )
                    
                    # è·å–ç»Ÿä¸€çš„ doc_idï¼ˆç”¨äº reranker èåˆï¼‰
                    # ä¼˜å…ˆä» metadata ä¸­è·å–ï¼Œå¦åˆ™ä½¿ç”¨ entity ä¸­çš„ doc_idï¼Œæœ€åä½¿ç”¨ result id
                    doc_id = metadata.get("doc_id") or entity.get("doc_id") or result.get("id", "unknown")
                    
                    search_results.append({
                        "id": result.get("id", "unknown"),
                        "doc_id": doc_id,  # æ·»åŠ  doc_id ç”¨äº reranker èåˆ
                        "content": content,
                        "file_path": file_path,
                        "chunk_index": metadata.get("chunk_index", 0),
                        "metadata": metadata,
                        "score": score,
                        "entity": entity,
                    })
            
            self.logger.info(f"ğŸ” Search completed, returned {len(search_results)} results")
            return search_results
            
        except Exception as e:
            self.logger.error(f"âŒ Search failed: {e}")
            return []
    
    async def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        if not self._client:
            return {"status": "not_initialized"}
        
        try:
            collections = self._client.list_collections()
            if self.config.collection_name not in collections:
                return {
                    "collection_name": self.config.collection_name,
                    "row_count": 0,
                    "status": "not_exists"
                }
            
            collection_info = self._client.describe_collection(self.config.collection_name)
            row_count = collection_info.get("num_rows", 0) or collection_info.get("row_count", 0)
            
            return {
                "collection_name": self.config.collection_name,
                "row_count": row_count,
                "vector_dim": self.config.vector_dim,
                "status": "exists",
            }
        except Exception as e:
            self.logger.error(f"âŒ Failed to get stats: {e}")
            return {"status": "error", "error": str(e)}
    
    async def delete_documents(self, document_ids: List[str]) -> int:
        """åˆ é™¤æ–‡æ¡£"""
        if not self._client or not document_ids:
            return 0
        
        try:
            self._client.delete(
                collection_name=self.config.collection_name,
                pks=document_ids
            )
            self.logger.info(f"âœ… Deleted {len(document_ids)} documents")
            return len(document_ids)
        except Exception as e:
            self.logger.error(f"âŒ Failed to delete: {e}")
            return 0
    
    async def get_chunks_by_file_path(self, file_path: str) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ–‡ä»¶è·¯å¾„è·å–æ‰€æœ‰ç›¸å…³çš„åˆ†å—
        
        Args:
            file_path: æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
            
        Returns:
            åˆ†å—åˆ—è¡¨ï¼Œæ¯ä¸ªåˆ†å—åŒ…å« id, content, metadata ç­‰ä¿¡æ¯
        """
        if not self._client:
            self.logger.error("Milvus client not initialized")
            return []
        
        results = []
        
        # å°è¯•å¤šç§æŸ¥è¯¢æ–¹å¼ï¼Œå› ä¸ºæ•°æ®å¯èƒ½ä»¥ä¸åŒæ–¹å¼å­˜å‚¨
        # 1. é¦–å…ˆå°è¯•ä½¿ç”¨åŠ¨æ€å­—æ®µæŸ¥è¯¢ï¼ˆLlamaIndex å­˜å‚¨æ—¶ metadata å­—æ®µä¼šè¢«æå‡åˆ°é¡¶å±‚ï¼‰
        try:
            filter_expr = f'relative_path == "{file_path}" or file_path == "{file_path}"'
            results = self._client.query(
                collection_name=self.config.collection_name,
                filter=filter_expr,
                output_fields=["*"],
                limit=1000
            )
            self.logger.debug(f"Dynamic field query returned {len(results)} results")
        except Exception as e:
            self.logger.debug(f"Dynamic field query failed: {e}")
        
        # 2. å¦‚æœæ²¡æœ‰ç»“æœï¼Œå°è¯•ä½¿ç”¨ id å‰ç¼€åŒ¹é…ï¼ˆdoc_id æ ¼å¼: relative_path#chunk_indexï¼‰
        if not results:
            try:
                # ä½¿ç”¨ like æ“ä½œç¬¦åŒ¹é…ä»¥ file_path# å¼€å¤´çš„ id
                filter_expr = f'id like "{file_path}#%"'
                results = self._client.query(
                    collection_name=self.config.collection_name,
                    filter=filter_expr,
                    output_fields=["*"],
                    limit=1000
                )
                self.logger.debug(f"ID prefix query returned {len(results)} results")
            except Exception as e:
                self.logger.debug(f"ID prefix query failed: {e}")
        
        # 3. å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œå°è¯•ä½¿ç”¨ pymilvus Collection API è¿›è¡Œæ›´çµæ´»çš„æŸ¥è¯¢
        if not results:
            try:
                from pymilvus import Collection
                collection = Collection(self.config.collection_name)
                collection.load()
                
                # å°è¯•ä¸åŒçš„è¡¨è¾¾å¼
                expressions = [
                    f'relative_path == "{file_path}"',
                    f'file_path == "{file_path}"',
                    f'id like "{file_path}#%"',
                ]
                
                for expr in expressions:
                    try:
                        query_results = collection.query(
                            expr=expr,
                            output_fields=["*"],
                            limit=1000
                        )
                        if query_results:
                            results = query_results
                            self.logger.debug(f"Collection query with '{expr}' returned {len(results)} results")
                            break
                    except Exception as e:
                        self.logger.debug(f"Query with '{expr}' failed: {e}")
                        continue
            except Exception as e:
                self.logger.debug(f"Collection API query failed: {e}")
        
        # æ ¼å¼åŒ–ç»“æœ
        chunks = []
        for result in results:
            # åŠ¨æ€å­—æ®µå­˜å‚¨åœ¨é¡¶å±‚ï¼Œä¸åœ¨ metadata åµŒå¥—å­—æ®µä¸­
            # æ„å»º metadata å­—å…¸ä»é¡¶å±‚å­—æ®µ
            metadata = {}
            metadata_fields = [
                'file_path', 'file_name', 'file_type', 'file_size',
                'creation_date', 'last_modified_date', 'absolute_path',
                'relative_path', 'chunk_index', 'chunk_id', 'doc_id'
            ]
            for field in metadata_fields:
                if field in result:
                    metadata[field] = result[field]
            
            # ä»å¤šä¸ªå¯èƒ½çš„ä½ç½®æå–å­—æ®µ
            content = result.get("content") or result.get("text", "")
            doc_id = result.get("doc_id") or result.get("id", "")
            result_file_path = result.get("relative_path") or result.get("file_path", "")
            chunk_index = result.get("chunk_index", 0)
            
            chunks.append({
                "id": result.get("id", "unknown"),
                "content": content,
                "doc_id": doc_id,
                "file_path": result_file_path,
                "chunk_index": chunk_index,
                "metadata": metadata,
                "entity": result,
            })
        
        self.logger.info(f"ğŸ” Found {len(chunks)} chunks for file: {file_path}")
        return chunks
    
    async def close(self) -> None:
        """å…³é—­è¿æ¥"""
        try:
            if self._client:
                self._client.close()
            connections.disconnect("default")
            self._initialized = False
            self.logger.info("âœ… Milvus connection closed")
        except Exception as e:
            self.logger.error(f"âŒ Failed to close: {e}")

